2017-03-18 00:29:10,550 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-18 00:29:10,559 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-18 00:29:10,564 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-18 00:29:10,900 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-18 00:29:10,991 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-18 00:29:10,991 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-18 00:29:10,994 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://192.168.43.6:8020
2017-03-18 00:29:10,994 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 192.168.43.6:8020 to access this namenode/service.
2017-03-18 00:29:11,595 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-18 00:29:11,658 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-18 00:29:11,668 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-18 00:29:11,675 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-18 00:29:11,681 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-18 00:29:11,686 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-18 00:29:11,686 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-18 00:29:11,686 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-18 00:29:11,947 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-18 00:29:11,951 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-18 00:29:11,975 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-18 00:29:11,975 INFO org.mortbay.log: jetty-6.1.26
2017-03-18 00:29:12,193 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-18 00:29:12,478 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-18 00:29:12,478 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-18 00:29:12,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-18 00:29:12,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-18 00:29:12,558 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-18 00:29:12,558 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-18 00:29:12,559 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-18 00:29:12,559 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 18 00:29:12
2017-03-18 00:29:12,561 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-18 00:29:12,561 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 00:29:12,563 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-18 00:29:12,563 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-18 00:29:12,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-18 00:29:12,569 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2017-03-18 00:29:12,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-18 00:29:12,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-18 00:29:12,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-18 00:29:12,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-18 00:29:12,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-18 00:29:12,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-18 00:29:12,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-18 00:29:12,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-18 00:29:12,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-18 00:29:12,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-18 00:29:12,577 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-18 00:29:12,618 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-18 00:29:12,618 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 00:29:12,618 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-18 00:29:12,618 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-18 00:29:12,620 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-18 00:29:12,620 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-18 00:29:12,620 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-18 00:29:12,620 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-18 00:29:12,627 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-18 00:29:12,627 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 00:29:12,627 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-18 00:29:12,627 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-18 00:29:12,629 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-18 00:29:12,629 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-18 00:29:12,629 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-18 00:29:12,631 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-18 00:29:12,631 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-18 00:29:12,631 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-18 00:29:12,633 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-18 00:29:12,633 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-18 00:29:12,659 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-18 00:29:12,659 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 00:29:12,659 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-18 00:29:12,659 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-18 00:29:12,717 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/hadoop2_data/hdfs/namenode/in_use.lock acquired by nodename 22731@gaurav-Inspiron-3542
2017-03-18 00:29:12,889 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/hadoop2_data/hdfs/namenode/current
2017-03-18 00:29:12,889 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2017-03-18 00:29:12,890 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-03-18 00:29:13,050 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-03-18 00:29:13,116 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-18 00:29:13,117 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000000
2017-03-18 00:29:13,125 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2017-03-18 00:29:13,125 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2017-03-18 00:29:13,132 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
2017-03-18 00:29:13,163 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 353 bytes saved in 0 seconds.
2017-03-18 00:29:13,242 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2017-03-18 00:29:13,296 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2017-03-18 00:29:13,464 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-18 00:29:13,464 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 801 msecs
2017-03-18 00:29:13,774 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to hadoop:8020
2017-03-18 00:29:13,782 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-18 00:29:13,793 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2017-03-18 00:29:13,847 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-18 00:29:13,948 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-18 00:29:13,948 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-18 00:29:13,948 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-18 00:29:13,949 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2017-03-18 00:29:13,949 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-03-18 00:29:13,949 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-18 00:29:13,982 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-18 00:29:13,983 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-03-18 00:29:13,983 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-18 00:29:13,983 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-18 00:29:13,983 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-18 00:29:13,983 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-18 00:29:13,983 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 35 msec
2017-03-18 00:29:14,048 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-18 00:29:14,048 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2017-03-18 00:29:14,052 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: hadoop/192.168.43.6:8020
2017-03-18 00:29:14,052 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-18 00:29:14,057 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-18 00:29:15,594 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0) storage 126b22ae-4c01-4c22-9c44-fc36c01c2f63
2017-03-18 00:29:15,594 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-18 00:29:15,594 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.43.6:50010
2017-03-18 00:29:15,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-18 00:29:15,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-463f3f03-e006-4115-83ce-563f6559d31a for DN 192.168.43.6:50010
2017-03-18 00:29:15,784 INFO BlockStateChange: BLOCK* processReport: from storage DS-463f3f03-e006-4115-83ce-563f6559d31a node DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2017-03-18 00:30:30,064 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.43.6
2017-03-18 00:30:30,064 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-18 00:30:30,064 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2017-03-18 00:30:30,065 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 83 
2017-03-18 00:30:30,073 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 91 
2017-03-18 00:30:30,074 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000001 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000001-0000000000000000002
2017-03-18 00:30:30,093 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2017-03-18 00:30:32,155 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2017-03-18 00:30:32,155 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 353 bytes.
2017-03-18 00:30:32,190 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2017-03-18 01:09:14,052 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* removeDeadDatanode: lost heartbeat from 192.168.43.6:50010
2017-03-18 01:09:14,171 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/192.168.43.6:50010
2017-03-18 10:03:24,790 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1155ms
No GCs detected
2017-03-18 10:08:11,755 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0) storage 126b22ae-4c01-4c22-9c44-fc36c01c2f63
2017-03-18 10:08:11,755 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/192.168.43.6:50010
2017-03-18 10:08:11,755 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.43.6:50010
2017-03-18 10:08:11,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-18 10:08:11,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: [DISK]DS-463f3f03-e006-4115-83ce-563f6559d31a:NORMAL:192.168.43.6:50010 failed.
2017-03-18 10:08:11,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage [DISK]DS-463f3f03-e006-4115-83ce-563f6559d31a:FAILED:192.168.43.6:50010 from DataNode192.168.43.6:50010
2017-03-18 10:08:11,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-18 10:08:11,758 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-463f3f03-e006-4115-83ce-563f6559d31a for DN 192.168.43.6:50010
2017-03-18 10:08:11,759 INFO BlockStateChange: BLOCK* processReport: from storage DS-463f3f03-e006-4115-83ce-563f6559d31a node DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2017-03-18 10:09:02,627 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-18 10:09:02,636 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-18 10:15:35,236 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-18 10:15:35,247 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-18 10:15:35,253 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-18 10:15:35,587 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-18 10:15:35,688 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-18 10:15:35,688 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-18 10:15:35,691 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://192.168.43.6:8020
2017-03-18 10:15:35,691 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 192.168.43.6:8020 to access this namenode/service.
2017-03-18 10:15:36,372 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-18 10:15:36,429 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-18 10:15:36,439 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-18 10:15:36,446 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-18 10:15:36,453 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-18 10:15:36,457 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-18 10:15:36,457 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-18 10:15:36,457 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-18 10:15:36,680 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-18 10:15:36,686 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-18 10:15:36,718 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-18 10:15:36,719 INFO org.mortbay.log: jetty-6.1.26
2017-03-18 10:15:36,930 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-18 10:15:37,279 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-18 10:15:37,280 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-18 10:15:37,331 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-18 10:15:37,331 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-18 10:15:37,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-18 10:15:37,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-18 10:15:37,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-18 10:15:37,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 18 10:15:37
2017-03-18 10:15:37,398 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-18 10:15:37,398 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 10:15:37,399 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-18 10:15:37,399 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-18 10:15:37,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-18 10:15:37,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2017-03-18 10:15:37,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-18 10:15:37,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-18 10:15:37,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-18 10:15:37,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-18 10:15:37,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-18 10:15:37,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-18 10:15:37,416 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-18 10:15:37,417 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-18 10:15:37,417 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-18 10:15:37,417 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-18 10:15:37,418 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-18 10:15:37,473 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-18 10:15:37,473 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 10:15:37,473 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-18 10:15:37,473 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-18 10:15:37,474 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-18 10:15:37,474 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-18 10:15:37,474 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-18 10:15:37,474 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-18 10:15:37,488 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-18 10:15:37,488 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 10:15:37,488 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-18 10:15:37,488 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-18 10:15:37,490 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-18 10:15:37,490 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-18 10:15:37,490 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-18 10:15:37,507 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-18 10:15:37,507 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-18 10:15:37,507 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-18 10:15:37,508 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-18 10:15:37,508 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-18 10:15:37,510 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-18 10:15:37,510 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 10:15:37,510 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-18 10:15:37,510 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-18 10:15:37,577 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/hadoop2_data/hdfs/namenode/in_use.lock acquired by nodename 26554@gaurav-Inspiron-3542
2017-03-18 10:15:37,722 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/hadoop2_data/hdfs/namenode/current
2017-03-18 10:15:37,812 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000003 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000003-0000000000000000003
2017-03-18 10:15:37,817 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2017-03-18 10:15:37,908 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-03-18 10:15:37,934 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-18 10:15:37,934 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2 from /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000002
2017-03-18 10:15:37,934 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7a7fdbb0 expecting start txid #3
2017-03-18 10:15:37,934 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000003-0000000000000000003
2017-03-18 10:15:37,936 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000003-0000000000000000003' to transaction ID 3
2017-03-18 10:15:37,938 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000003-0000000000000000003 of size 1048576 edits # 1 loaded in 0 seconds
2017-03-18 10:15:37,942 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2017-03-18 10:15:37,942 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2017-03-18 10:15:37,947 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage.ckpt_0000000000000000003 using no compression
2017-03-18 10:15:37,994 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage.ckpt_0000000000000000003 of size 353 bytes saved in 0 seconds.
2017-03-18 10:15:38,046 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2017-03-18 10:15:38,047 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-03-18 10:15:38,112 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4
2017-03-18 10:15:38,268 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-18 10:15:38,269 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 756 msecs
2017-03-18 10:15:38,530 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to hadoop:8020
2017-03-18 10:15:38,536 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-18 10:15:38,546 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2017-03-18 10:15:38,596 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-18 10:15:38,617 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-18 10:15:38,617 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-18 10:15:38,617 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-18 10:15:38,618 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2017-03-18 10:15:38,618 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-03-18 10:15:38,618 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-18 10:15:38,624 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-18 10:15:38,624 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-03-18 10:15:38,624 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-18 10:15:38,624 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-18 10:15:38,624 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-18 10:15:38,624 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-18 10:15:38,625 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 7 msec
2017-03-18 10:15:38,702 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-18 10:15:38,702 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2017-03-18 10:15:38,707 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: hadoop/192.168.43.6:8020
2017-03-18 10:15:38,707 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-18 10:15:38,724 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-18 10:16:51,609 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.43.6
2017-03-18 10:16:51,610 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-18 10:16:51,610 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 4
2017-03-18 10:16:51,610 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 85 
2017-03-18 10:16:51,625 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 101 
2017-03-18 10:16:51,626 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000004 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000004-0000000000000000005
2017-03-18 10:16:51,626 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 6
2017-03-18 10:16:53,103 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2017-03-18 10:16:53,103 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000005 size 353 bytes.
2017-03-18 10:16:53,137 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3
2017-03-18 10:16:53,138 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2017-03-18 10:23:19,638 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-18 10:23:19,640 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-18 10:24:22,121 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-18 10:24:22,132 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-18 10:24:22,137 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-18 10:24:22,492 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-18 10:24:22,585 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-18 10:24:22,585 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-18 10:24:22,589 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://192.168.43.6:8020
2017-03-18 10:24:22,589 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 192.168.43.6:8020 to access this namenode/service.
2017-03-18 10:24:23,158 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-18 10:24:23,221 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-18 10:24:23,231 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-18 10:24:23,238 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-18 10:24:23,244 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-18 10:24:23,249 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-18 10:24:23,249 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-18 10:24:23,249 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-18 10:24:23,391 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-18 10:24:23,392 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-18 10:24:23,408 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-18 10:24:23,409 INFO org.mortbay.log: jetty-6.1.26
2017-03-18 10:24:23,599 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-18 10:24:23,834 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-18 10:24:23,834 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-18 10:24:23,872 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-18 10:24:23,872 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-18 10:24:23,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-18 10:24:23,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-18 10:24:23,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-18 10:24:23,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 18 10:24:23
2017-03-18 10:24:23,915 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-18 10:24:23,915 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 10:24:23,916 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-18 10:24:23,916 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-18 10:24:23,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-18 10:24:23,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2017-03-18 10:24:23,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-18 10:24:23,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-18 10:24:23,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-18 10:24:23,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-18 10:24:23,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-18 10:24:23,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-18 10:24:23,929 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-18 10:24:23,929 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-18 10:24:23,929 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-18 10:24:23,929 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-18 10:24:23,930 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-18 10:24:23,969 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-18 10:24:23,969 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 10:24:23,970 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-18 10:24:23,970 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-18 10:24:23,971 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-18 10:24:23,971 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-18 10:24:23,971 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-18 10:24:23,971 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-18 10:24:23,977 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-18 10:24:23,977 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 10:24:23,977 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-18 10:24:23,977 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-18 10:24:23,979 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-18 10:24:23,979 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-18 10:24:23,979 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-18 10:24:23,982 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-18 10:24:23,982 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-18 10:24:23,982 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-18 10:24:23,983 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-18 10:24:23,983 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-18 10:24:23,985 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-18 10:24:23,985 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 10:24:23,985 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-18 10:24:23,985 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-18 10:24:24,068 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/hadoop2_data/hdfs/namenode/in_use.lock acquired by nodename 27892@gaurav-Inspiron-3542
2017-03-18 10:24:24,179 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/hadoop2_data/hdfs/namenode/current
2017-03-18 10:24:24,240 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000006 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000006-0000000000000000006
2017-03-18 10:24:24,247 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000005, cpktTxId=0000000000000000005)
2017-03-18 10:24:24,317 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-03-18 10:24:24,349 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-18 10:24:24,349 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 5 from /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000005
2017-03-18 10:24:24,349 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7a7fdbb0 expecting start txid #6
2017-03-18 10:24:24,350 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000006-0000000000000000006
2017-03-18 10:24:24,351 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000006-0000000000000000006' to transaction ID 6
2017-03-18 10:24:24,354 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000006-0000000000000000006 of size 1048576 edits # 1 loaded in 0 seconds
2017-03-18 10:24:24,359 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-03-18 10:24:24,361 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 7
2017-03-18 10:24:24,537 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-18 10:24:24,537 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 550 msecs
2017-03-18 10:24:24,698 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to hadoop:8020
2017-03-18 10:24:24,704 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-18 10:24:24,714 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2017-03-18 10:24:24,766 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-18 10:24:24,773 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-18 10:24:24,774 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-18 10:24:24,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-18 10:24:24,774 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2017-03-18 10:24:24,774 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-03-18 10:24:24,774 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-18 10:24:24,787 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-18 10:24:24,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-03-18 10:24:24,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-18 10:24:24,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-18 10:24:24,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-18 10:24:24,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-18 10:24:24,788 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 14 msec
2017-03-18 10:24:24,831 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-18 10:24:24,831 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2017-03-18 10:24:24,842 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: hadoop/192.168.43.6:8020
2017-03-18 10:24:24,842 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-18 10:24:24,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-18 10:25:37,046 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.43.6
2017-03-18 10:25:37,046 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-18 10:25:37,046 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 7
2017-03-18 10:25:37,047 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 106 
2017-03-18 10:25:37,067 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 126 
2017-03-18 10:25:37,068 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000007 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000007-0000000000000000008
2017-03-18 10:25:37,068 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 9
2017-03-18 10:25:38,537 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2017-03-18 10:25:38,538 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000008 size 353 bytes.
2017-03-18 10:25:38,574 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 5
2017-03-18 10:25:38,574 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000003, cpktTxId=0000000000000000003)
2017-03-18 11:25:39,319 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.43.6
2017-03-18 11:25:39,366 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-18 11:25:39,366 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 9
2017-03-18 11:25:39,402 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 36 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 66 
2017-03-18 11:25:39,460 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 36 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 112 
2017-03-18 11:25:39,474 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000009 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000009-0000000000000000010
2017-03-18 11:25:39,490 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 11
2017-03-18 11:25:40,505 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.09s at 0.00 KB/s
2017-03-18 11:25:40,505 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000010 size 353 bytes.
2017-03-18 11:25:40,595 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 8
2017-03-18 11:25:40,595 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000005, cpktTxId=0000000000000000005)
2017-03-18 12:25:42,063 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.43.6
2017-03-18 12:25:42,075 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-18 12:25:42,075 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 11
2017-03-18 12:25:42,076 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 75 
2017-03-18 12:25:42,112 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 111 
2017-03-18 12:25:42,113 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000011 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000011-0000000000000000012
2017-03-18 12:25:42,113 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 13
2017-03-18 12:25:42,639 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2017-03-18 12:25:42,639 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000012 size 353 bytes.
2017-03-18 12:25:42,698 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 10
2017-03-18 12:25:42,698 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000008, cpktTxId=0000000000000000008)
2017-03-18 13:22:56,842 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 162 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 85 
2017-03-18 13:25:43,437 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.43.6
2017-03-18 13:25:43,437 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-18 13:25:43,437 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 13
2017-03-18 13:25:43,438 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 8 Total time for transactions(ms): 164 Number of transactions batched in Syncs: 0 Number of syncs: 7 SyncTimes(ms): 291 
2017-03-18 13:25:43,464 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 8 Total time for transactions(ms): 164 Number of transactions batched in Syncs: 0 Number of syncs: 8 SyncTimes(ms): 316 
2017-03-18 13:25:43,472 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000013 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000013-0000000000000000020
2017-03-18 13:25:43,483 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 21
2017-03-18 13:25:44,308 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2017-03-18 13:25:44,308 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000020 size 557 bytes.
2017-03-18 13:25:44,352 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 12
2017-03-18 13:25:44,352 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000010, cpktTxId=0000000000000000010)
2017-03-18 13:33:45,236 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1199ms
No GCs detected
2017-03-18 14:09:45,317 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1218ms
No GCs detected
2017-03-18 14:47:58,444 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-18 14:47:58,445 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-18 14:48:50,934 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-18 14:48:50,945 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-18 14:48:50,951 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-18 14:48:51,432 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-18 14:48:51,574 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-18 14:48:51,574 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-18 14:48:51,578 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://192.168.43.6:8020
2017-03-18 14:48:51,578 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 192.168.43.6:8020 to access this namenode/service.
2017-03-18 14:48:52,072 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-18 14:48:52,167 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-18 14:48:52,204 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-18 14:48:52,209 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-18 14:48:52,224 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-18 14:48:52,228 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-18 14:48:52,228 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-18 14:48:52,228 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-18 14:48:52,516 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-18 14:48:52,522 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-18 14:48:52,595 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-18 14:48:52,595 INFO org.mortbay.log: jetty-6.1.26
2017-03-18 14:48:52,943 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-18 14:48:53,041 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-18 14:48:53,041 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-18 14:48:53,100 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-18 14:48:53,100 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-18 14:48:53,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-18 14:48:53,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-18 14:48:53,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-18 14:48:53,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 18 14:48:53
2017-03-18 14:48:53,196 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-18 14:48:53,196 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 14:48:53,215 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-18 14:48:53,215 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-18 14:48:53,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-18 14:48:53,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2017-03-18 14:48:53,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-18 14:48:53,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-18 14:48:53,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-18 14:48:53,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-18 14:48:53,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-18 14:48:53,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-18 14:48:53,243 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-18 14:48:53,243 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-18 14:48:53,243 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-18 14:48:53,243 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-18 14:48:53,244 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-18 14:48:53,453 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-18 14:48:53,453 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 14:48:53,454 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-18 14:48:53,454 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-18 14:48:53,455 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-18 14:48:53,455 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-18 14:48:53,455 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-18 14:48:53,456 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-18 14:48:53,478 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-18 14:48:53,478 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 14:48:53,478 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-18 14:48:53,478 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-18 14:48:53,480 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-18 14:48:53,481 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-18 14:48:53,481 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-18 14:48:53,498 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-18 14:48:53,499 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-18 14:48:53,499 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-18 14:48:53,500 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-18 14:48:53,500 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-18 14:48:53,510 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-18 14:48:53,510 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-18 14:48:53,510 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-18 14:48:53,510 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-18 14:48:53,551 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/hadoop2_data/hdfs/namenode/in_use.lock acquired by nodename 4089@gaurav-Inspiron-3542
2017-03-18 14:48:53,692 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/hadoop2_data/hdfs/namenode/current
2017-03-18 14:48:53,779 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000021 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000021-0000000000000000021
2017-03-18 14:48:53,788 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000020, cpktTxId=0000000000000000020)
2017-03-18 14:48:53,942 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 4 INodes.
2017-03-18 14:48:53,969 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-18 14:48:53,970 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 20 from /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000020
2017-03-18 14:48:53,970 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@71b59bf6 expecting start txid #21
2017-03-18 14:48:53,970 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000021-0000000000000000021
2017-03-18 14:48:53,971 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000021-0000000000000000021' to transaction ID 21
2017-03-18 14:48:53,974 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000021-0000000000000000021 of size 1048576 edits # 1 loaded in 0 seconds
2017-03-18 14:48:53,994 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2017-03-18 14:48:53,994 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2017-03-18 14:48:53,999 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage.ckpt_0000000000000000021 using no compression
2017-03-18 14:48:54,033 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage.ckpt_0000000000000000021 of size 557 bytes saved in 0 seconds.
2017-03-18 14:48:54,098 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 20
2017-03-18 14:48:54,098 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000012, cpktTxId=0000000000000000012)
2017-03-18 14:48:54,166 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 22
2017-03-18 14:48:54,309 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-18 14:48:54,309 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 796 msecs
2017-03-18 14:48:54,672 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to hadoop:8020
2017-03-18 14:48:54,678 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-18 14:48:54,693 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2017-03-18 14:48:54,991 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-18 14:48:55,020 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-18 14:48:55,021 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-18 14:48:55,021 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-18 14:48:55,021 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2017-03-18 14:48:55,021 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-03-18 14:48:55,021 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-18 14:48:55,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-18 14:48:55,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-03-18 14:48:55,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-18 14:48:55,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-18 14:48:55,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-18 14:48:55,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-18 14:48:55,029 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 9 msec
2017-03-18 14:48:55,123 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-18 14:48:55,124 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2017-03-18 14:48:55,126 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: hadoop/192.168.43.6:8020
2017-03-18 14:48:55,126 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-18 14:48:55,143 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-18 14:50:08,713 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.43.6
2017-03-18 14:50:08,713 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-18 14:50:08,713 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 22
2017-03-18 14:50:08,713 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 74 
2017-03-18 14:50:08,727 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 89 
2017-03-18 14:50:08,728 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000022 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000022-0000000000000000023
2017-03-18 14:50:08,728 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 24
2017-03-18 14:50:10,376 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2017-03-18 14:50:10,376 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000023 size 557 bytes.
2017-03-18 14:50:10,411 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 21
2017-03-18 14:50:10,411 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000020, cpktTxId=0000000000000000020)
2017-03-18 14:50:40,016 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-18 14:50:40,018 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-22 13:13:52,806 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-22 13:13:52,893 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-22 13:13:52,897 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-22 13:13:53,312 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-22 13:13:53,409 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-22 13:13:53,409 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-22 13:13:53,412 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://192.168.43.204:8020
2017-03-22 13:13:53,412 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 192.168.43.204:8020 to access this namenode/service.
2017-03-22 13:13:53,942 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-22 13:13:54,114 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-22 13:13:54,131 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-22 13:13:54,138 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-22 13:13:54,146 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-22 13:13:54,152 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-22 13:13:54,152 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-22 13:13:54,152 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-22 13:13:54,344 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-22 13:13:54,354 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-22 13:13:54,401 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-22 13:13:54,401 INFO org.mortbay.log: jetty-6.1.26
2017-03-22 13:13:54,598 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-22 13:13:54,655 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 13:13:54,655 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 13:13:54,718 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-22 13:13:54,718 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-22 13:13:54,780 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-22 13:13:54,780 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-22 13:13:54,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-22 13:13:54,782 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 22 13:13:54
2017-03-22 13:13:54,783 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-22 13:13:54,783 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:13:54,785 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-22 13:13:54,785 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-22 13:13:54,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-22 13:13:54,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2017-03-22 13:13:54,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-22 13:13:54,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-22 13:13:54,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-22 13:13:54,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-22 13:13:54,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-22 13:13:54,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-22 13:13:54,797 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-22 13:13:54,797 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-22 13:13:54,797 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-22 13:13:54,797 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-22 13:13:54,799 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-22 13:13:54,952 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-22 13:13:54,952 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:13:54,952 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-22 13:13:54,952 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-22 13:13:54,953 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-22 13:13:54,953 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-22 13:13:54,953 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-22 13:13:54,953 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-22 13:13:54,975 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-22 13:13:54,975 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:13:54,975 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-22 13:13:54,975 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-22 13:13:54,976 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-22 13:13:54,976 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-22 13:13:54,976 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-22 13:13:54,986 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-22 13:13:54,986 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-22 13:13:54,986 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-22 13:13:54,987 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-22 13:13:54,987 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-22 13:13:54,989 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-22 13:13:54,989 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:13:54,989 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-22 13:13:54,989 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-22 13:13:55,067 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/hadoop2_data/hdfs/namenode/in_use.lock acquired by nodename 3469@gaurav-Inspiron-3542
2017-03-22 13:13:55,211 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/hadoop2_data/hdfs/namenode/current
2017-03-22 13:13:55,320 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000024 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000024-0000000000000000024
2017-03-22 13:13:55,341 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000023, cpktTxId=0000000000000000023)
2017-03-22 13:13:55,490 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 4 INodes.
2017-03-22 13:13:55,527 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-22 13:13:55,527 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 23 from /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000023
2017-03-22 13:13:55,527 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7fe69211 expecting start txid #24
2017-03-22 13:13:55,527 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000024-0000000000000000024
2017-03-22 13:13:55,529 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000024-0000000000000000024' to transaction ID 24
2017-03-22 13:13:55,531 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000024-0000000000000000024 of size 1048576 edits # 1 loaded in 0 seconds
2017-03-22 13:13:55,549 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2017-03-22 13:13:55,549 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2017-03-22 13:13:55,554 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage.ckpt_0000000000000000024 using no compression
2017-03-22 13:13:55,587 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage.ckpt_0000000000000000024 of size 557 bytes saved in 0 seconds.
2017-03-22 13:13:55,637 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 23
2017-03-22 13:13:55,637 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000021, cpktTxId=0000000000000000021)
2017-03-22 13:13:55,683 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 25
2017-03-22 13:13:55,836 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-22 13:13:55,836 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 844 msecs
2017-03-22 13:13:56,088 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to hadoop:8020
2017-03-22 13:13:56,104 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-22 13:13:56,125 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2017-03-22 13:13:56,257 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-22 13:13:56,263 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-22 13:13:56,263 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-22 13:13:56,264 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-22 13:13:56,264 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2017-03-22 13:13:56,264 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-03-22 13:13:56,264 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-22 13:13:56,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:13:56,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-03-22 13:13:56,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-22 13:13:56,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-22 13:13:56,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-22 13:13:56,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-22 13:13:56,271 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 8 msec
2017-03-22 13:13:56,315 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-22 13:13:56,315 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2017-03-22 13:13:56,318 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: hadoop/192.168.43.6:8020
2017-03-22 13:13:56,318 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-22 13:13:56,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-22 13:14:03,431 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0) storage 126b22ae-4c01-4c22-9c44-fc36c01c2f63
2017-03-22 13:14:03,431 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:14:03,432 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.43.6:50010
2017-03-22 13:14:03,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:14:03,525 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-463f3f03-e006-4115-83ce-563f6559d31a for DN 192.168.43.6:50010
2017-03-22 13:14:03,560 INFO BlockStateChange: BLOCK* processReport: from storage DS-463f3f03-e006-4115-83ce-563f6559d31a node DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2017-03-22 13:15:08,852 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-22 13:15:08,854 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-22 13:18:43,444 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-22 13:18:43,452 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-22 13:18:43,456 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-22 13:18:43,788 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-22 13:18:43,877 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-22 13:18:43,877 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-22 13:18:43,880 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://192.168.43.204:8020
2017-03-22 13:18:43,880 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 192.168.43.204:8020 to access this namenode/service.
2017-03-22 13:18:44,214 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-22 13:18:44,286 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-22 13:18:44,301 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-22 13:18:44,311 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-22 13:18:44,321 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-22 13:18:44,327 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-22 13:18:44,328 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-22 13:18:44,328 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-22 13:18:44,489 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-22 13:18:44,490 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-22 13:18:44,505 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-22 13:18:44,505 INFO org.mortbay.log: jetty-6.1.26
2017-03-22 13:18:44,691 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-22 13:18:44,724 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 13:18:44,724 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 13:18:44,771 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-22 13:18:44,771 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-22 13:18:44,812 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-22 13:18:44,812 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-22 13:18:44,813 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-22 13:18:44,813 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 22 13:18:44
2017-03-22 13:18:44,815 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-22 13:18:44,815 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:18:44,816 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-22 13:18:44,816 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-22 13:18:44,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-22 13:18:44,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2017-03-22 13:18:44,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-22 13:18:44,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-22 13:18:44,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-22 13:18:44,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-22 13:18:44,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-22 13:18:44,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-22 13:18:44,831 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-22 13:18:44,831 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-22 13:18:44,831 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-22 13:18:44,831 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-22 13:18:44,833 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-22 13:18:44,871 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-22 13:18:44,871 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:18:44,871 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-22 13:18:44,871 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-22 13:18:44,872 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-22 13:18:44,872 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-22 13:18:44,872 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-22 13:18:44,873 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-22 13:18:44,879 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-22 13:18:44,879 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:18:44,879 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-22 13:18:44,879 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-22 13:18:44,881 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-22 13:18:44,881 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-22 13:18:44,881 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-22 13:18:44,883 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-22 13:18:44,884 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-22 13:18:44,884 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-22 13:18:44,885 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-22 13:18:44,885 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-22 13:18:44,886 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-22 13:18:44,887 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:18:44,887 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-22 13:18:44,887 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-22 13:18:44,957 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/hadoop2_data/hdfs/namenode/in_use.lock acquired by nodename 5113@gaurav-Inspiron-3542
2017-03-22 13:18:45,038 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/hadoop2_data/hdfs/namenode/current
2017-03-22 13:18:45,098 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000025 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000025-0000000000000000025
2017-03-22 13:18:45,107 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000024, cpktTxId=0000000000000000024)
2017-03-22 13:18:45,175 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 4 INodes.
2017-03-22 13:18:45,216 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-22 13:18:45,217 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 24 from /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000024
2017-03-22 13:18:45,217 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7a7fdbb0 expecting start txid #25
2017-03-22 13:18:45,217 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000025-0000000000000000025
2017-03-22 13:18:45,220 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000025-0000000000000000025' to transaction ID 25
2017-03-22 13:18:45,224 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000025-0000000000000000025 of size 1048576 edits # 1 loaded in 0 seconds
2017-03-22 13:18:45,231 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-03-22 13:18:45,235 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 26
2017-03-22 13:18:45,425 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-22 13:18:45,425 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 535 msecs
2017-03-22 13:18:45,565 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to hadoop:8020
2017-03-22 13:18:45,570 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-22 13:18:45,580 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2017-03-22 13:18:45,629 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-22 13:18:45,635 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-22 13:18:45,635 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-22 13:18:45,635 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-22 13:18:45,635 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2017-03-22 13:18:45,635 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-03-22 13:18:45,635 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-22 13:18:45,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:18:45,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-03-22 13:18:45,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-22 13:18:45,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-22 13:18:45,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-22 13:18:45,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-22 13:18:45,644 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 10 msec
2017-03-22 13:18:45,675 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-22 13:18:45,675 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2017-03-22 13:18:45,678 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: hadoop/192.168.43.6:8020
2017-03-22 13:18:45,678 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-22 13:18:45,681 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-22 13:19:10,307 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.43.107
2017-03-22 13:19:10,307 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-22 13:19:10,307 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 26
2017-03-22 13:19:10,329 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 124 
2017-03-22 13:19:10,331 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000026 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000026-0000000000000000027
2017-03-22 13:19:10,331 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 28
2017-03-22 13:19:15,336 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2017-03-22 13:19:15,336 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000027 size 557 bytes.
2017-03-22 13:19:15,383 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 24
2017-03-22 13:19:15,383 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000023, cpktTxId=0000000000000000023)
2017-03-22 13:19:31,416 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0) storage 126b22ae-4c01-4c22-9c44-fc36c01c2f63
2017-03-22 13:19:31,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:19:31,417 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.43.6:50010
2017-03-22 13:19:31,493 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:19:31,493 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-463f3f03-e006-4115-83ce-563f6559d31a for DN 192.168.43.6:50010
2017-03-22 13:19:31,536 INFO BlockStateChange: BLOCK* processReport: from storage DS-463f3f03-e006-4115-83ce-563f6559d31a node DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2017-03-22 13:20:14,169 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 60 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 87 
2017-03-22 13:22:52,394 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-22 13:22:52,396 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-22 13:25:48,568 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-22 13:25:48,577 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-22 13:25:48,582 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-22 13:25:49,004 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-22 13:25:49,117 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-22 13:25:49,117 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-22 13:25:49,119 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://192.168.43.6:8020
2017-03-22 13:25:49,120 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 192.168.43.6:8020 to access this namenode/service.
2017-03-22 13:25:50,943 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-22 13:25:51,022 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-22 13:25:51,042 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-22 13:25:51,053 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-22 13:25:51,068 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-22 13:25:51,074 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-22 13:25:51,074 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-22 13:25:51,075 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-22 13:25:51,259 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-22 13:25:51,261 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-22 13:25:51,281 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-22 13:25:51,281 INFO org.mortbay.log: jetty-6.1.26
2017-03-22 13:25:51,534 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-22 13:25:51,593 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 13:25:51,593 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 13:25:51,661 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-22 13:25:51,661 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-22 13:25:51,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-22 13:25:51,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-22 13:25:51,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-22 13:25:51,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 22 13:25:51
2017-03-22 13:25:51,722 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-22 13:25:51,722 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:25:51,723 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-22 13:25:51,724 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-22 13:25:51,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-22 13:25:51,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2017-03-22 13:25:51,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-22 13:25:51,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-22 13:25:51,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-22 13:25:51,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-22 13:25:51,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-22 13:25:51,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-22 13:25:51,743 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-22 13:25:51,743 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-22 13:25:51,743 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-22 13:25:51,743 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-22 13:25:51,745 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-22 13:25:51,798 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-22 13:25:51,798 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:25:51,799 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-22 13:25:51,799 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-22 13:25:51,800 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-22 13:25:51,800 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-22 13:25:51,800 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-22 13:25:51,801 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-22 13:25:51,811 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-22 13:25:51,811 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:25:51,812 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-22 13:25:51,812 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-22 13:25:51,817 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-22 13:25:51,817 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-22 13:25:51,817 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-22 13:25:51,821 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-22 13:25:51,822 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-22 13:25:51,822 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-22 13:25:51,824 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-22 13:25:51,824 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-22 13:25:51,826 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-22 13:25:51,826 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:25:51,826 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-22 13:25:51,826 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-22 13:25:51,870 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/hadoop2_data/hdfs/namenode/in_use.lock acquired by nodename 6968@gaurav-Inspiron-3542
2017-03-22 13:25:51,967 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/hadoop2_data/hdfs/namenode/current
2017-03-22 13:25:52,245 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000028 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000028-0000000000000000030
2017-03-22 13:25:52,258 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000027, cpktTxId=0000000000000000027)
2017-03-22 13:25:52,310 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 4 INodes.
2017-03-22 13:25:52,360 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-22 13:25:52,360 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 27 from /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000027
2017-03-22 13:25:52,360 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2868ab35 expecting start txid #28
2017-03-22 13:25:52,360 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000028-0000000000000000030
2017-03-22 13:25:52,362 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000028-0000000000000000030' to transaction ID 28
2017-03-22 13:25:52,376 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000028-0000000000000000030 of size 1048576 edits # 3 loaded in 0 seconds
2017-03-22 13:25:52,376 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-03-22 13:25:52,380 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 31
2017-03-22 13:25:52,516 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-22 13:25:52,516 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 687 msecs
2017-03-22 13:25:52,716 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to hadoop:8020
2017-03-22 13:25:52,723 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-22 13:25:52,732 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2017-03-22 13:25:52,797 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-22 13:25:52,805 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-22 13:25:52,806 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-22 13:25:52,806 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-22 13:25:52,806 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2017-03-22 13:25:52,806 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-03-22 13:25:52,806 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-22 13:25:52,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:25:52,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-03-22 13:25:52,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-22 13:25:52,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-22 13:25:52,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-22 13:25:52,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-22 13:25:52,820 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 14 msec
2017-03-22 13:25:52,860 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-22 13:25:52,863 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2017-03-22 13:25:52,869 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: hadoop/192.168.43.6:8020
2017-03-22 13:25:52,869 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-22 13:25:52,897 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-22 13:25:53,879 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0) storage 126b22ae-4c01-4c22-9c44-fc36c01c2f63
2017-03-22 13:25:53,879 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:25:53,880 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.43.6:50010
2017-03-22 13:25:53,889 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:25:53,890 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-463f3f03-e006-4115-83ce-563f6559d31a for DN 192.168.43.6:50010
2017-03-22 13:25:53,910 INFO BlockStateChange: BLOCK* processReport: from storage DS-463f3f03-e006-4115-83ce-563f6559d31a node DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2017-03-22 13:26:53,454 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 62 
2017-03-22 13:28:43,476 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 78 
2017-03-22 13:31:01,337 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 6 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 0 Number of syncs: 6 SyncTimes(ms): 146 
2017-03-22 13:33:02,121 INFO org.apache.hadoop.ipc.Server: Connection from 192.168.43.6:50584 for protocol org.apache.hadoop.hdfs.protocol.ClientProtocol is unauthorized for user hadoop (auth:PROXY) via hadoop (auth:SIMPLE)
2017-03-22 13:33:02,122 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8020: readAndProcess from client 192.168.43.6 threw exception [org.apache.hadoop.security.authorize.AuthorizationException: User: hadoop is not allowed to impersonate hadoop]
2017-03-22 13:36:42,310 INFO org.apache.hadoop.ipc.Server: Connection from 192.168.43.6:50631 for protocol org.apache.hadoop.hdfs.protocol.ClientProtocol is unauthorized for user hadoop (auth:PROXY) via hadoop (auth:SIMPLE)
2017-03-22 13:36:42,310 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8020: readAndProcess from client 192.168.43.6 threw exception [org.apache.hadoop.security.authorize.AuthorizationException: User: hadoop is not allowed to impersonate hadoop]
2017-03-22 13:40:10,462 INFO org.apache.hadoop.ipc.Server: Connection from 192.168.43.6:50639 for protocol org.apache.hadoop.hdfs.protocol.ClientProtocol is unauthorized for user hadoop (auth:PROXY) via hadoop (auth:SIMPLE)
2017-03-22 13:40:10,462 INFO org.apache.hadoop.ipc.Server: Socket Reader #1 for port 8020: readAndProcess from client 192.168.43.6 threw exception [org.apache.hadoop.security.authorize.AuthorizationException: User: hadoop is not allowed to impersonate hadoop]
2017-03-22 13:40:22,856 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-22 13:40:22,858 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-22 13:42:46,523 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-22 13:42:46,531 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-22 13:42:46,536 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-22 13:42:46,936 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-22 13:42:47,036 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-22 13:42:47,036 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-22 13:42:47,039 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://192.168.43.6:8020
2017-03-22 13:42:47,039 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 192.168.43.6:8020 to access this namenode/service.
2017-03-22 13:42:47,650 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-22 13:42:47,725 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-22 13:42:47,739 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-22 13:42:47,748 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-22 13:42:47,758 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-22 13:42:47,763 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-22 13:42:47,763 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-22 13:42:47,764 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-22 13:42:47,963 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-22 13:42:47,982 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-22 13:42:48,041 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-22 13:42:48,042 INFO org.mortbay.log: jetty-6.1.26
2017-03-22 13:42:48,260 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-22 13:42:48,561 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 13:42:48,561 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 13:42:48,597 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-22 13:42:48,597 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-22 13:42:48,645 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-22 13:42:48,645 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-22 13:42:48,646 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-22 13:42:48,646 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 22 13:42:48
2017-03-22 13:42:48,648 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-22 13:42:48,648 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:42:48,649 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-22 13:42:48,649 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-22 13:42:48,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-22 13:42:48,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2017-03-22 13:42:48,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-22 13:42:48,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-22 13:42:48,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-22 13:42:48,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-22 13:42:48,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-22 13:42:48,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-22 13:42:48,662 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-22 13:42:48,662 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-22 13:42:48,662 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-22 13:42:48,663 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-22 13:42:48,664 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-22 13:42:48,747 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-22 13:42:48,747 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:42:48,747 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-22 13:42:48,747 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-22 13:42:48,748 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-22 13:42:48,748 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-22 13:42:48,748 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-22 13:42:48,748 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-22 13:42:48,755 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-22 13:42:48,755 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:42:48,755 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-22 13:42:48,755 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-22 13:42:48,756 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-22 13:42:48,757 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-22 13:42:48,757 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-22 13:42:48,759 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-22 13:42:48,759 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-22 13:42:48,759 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-22 13:42:48,760 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-22 13:42:48,760 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-22 13:42:48,762 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-22 13:42:48,762 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 13:42:48,762 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-22 13:42:48,762 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-22 13:42:48,822 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/hadoop2_data/hdfs/namenode/in_use.lock acquired by nodename 10354@gaurav-Inspiron-3542
2017-03-22 13:42:48,923 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/hadoop2_data/hdfs/namenode/current
2017-03-22 13:42:49,070 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_inprogress_0000000000000000031 -> /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000031-0000000000000000038
2017-03-22 13:42:49,093 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000027, cpktTxId=0000000000000000027)
2017-03-22 13:42:49,132 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 4 INodes.
2017-03-22 13:42:49,167 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-22 13:42:49,167 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 27 from /home/hadoop/hadoop2_data/hdfs/namenode/current/fsimage_0000000000000000027
2017-03-22 13:42:49,167 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5c5a3b32 expecting start txid #28
2017-03-22 13:42:49,167 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000028-0000000000000000030
2017-03-22 13:42:49,169 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000028-0000000000000000030' to transaction ID 28
2017-03-22 13:42:49,210 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000028-0000000000000000030 of size 1048576 edits # 3 loaded in 0 seconds
2017-03-22 13:42:49,210 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@27e47efc expecting start txid #31
2017-03-22 13:42:49,210 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000031-0000000000000000038
2017-03-22 13:42:49,210 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000031-0000000000000000038' to transaction ID 28
2017-03-22 13:42:49,235 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/hadoop2_data/hdfs/namenode/current/edits_0000000000000000031-0000000000000000038 of size 1048576 edits # 8 loaded in 0 seconds
2017-03-22 13:42:49,236 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-03-22 13:42:49,239 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 39
2017-03-22 13:42:49,368 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-22 13:42:49,368 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 603 msecs
2017-03-22 13:42:49,554 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to hadoop:8020
2017-03-22 13:42:49,559 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-22 13:42:49,570 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8020
2017-03-22 13:42:49,660 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-22 13:42:49,680 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-22 13:42:49,680 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-22 13:42:49,680 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-22 13:42:49,680 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2017-03-22 13:42:49,680 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-03-22 13:42:49,680 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-22 13:42:49,687 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:42:49,689 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-03-22 13:42:49,689 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-22 13:42:49,689 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-22 13:42:49,689 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-22 13:42:49,689 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-22 13:42:49,689 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 10 msec
2017-03-22 13:42:49,718 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-22 13:42:49,718 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8020: starting
2017-03-22 13:42:49,720 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: hadoop/192.168.43.6:8020
2017-03-22 13:42:49,721 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-22 13:42:49,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-22 13:42:50,282 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0) storage 126b22ae-4c01-4c22-9c44-fc36c01c2f63
2017-03-22 13:42:50,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:42:50,283 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.43.6:50010
2017-03-22 13:42:50,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 13:42:50,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-463f3f03-e006-4115-83ce-563f6559d31a for DN 192.168.43.6:50010
2017-03-22 13:42:50,304 INFO BlockStateChange: BLOCK* processReport: from storage DS-463f3f03-e006-4115-83ce-563f6559d31a node DatanodeRegistration(192.168.43.6:50010, datanodeUuid=126b22ae-4c01-4c22-9c44-fc36c01c2f63, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-431f5763-0573-43ff-bfe3-45c77d9adf25;nsid=2021752597;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2017-03-22 13:44:02,346 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 63 
2017-03-22 13:45:27,439 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 88 
2017-03-22 13:46:54,169 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-22 13:46:54,171 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-22 18:51:40,447 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-22 18:51:40,455 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-22 18:51:40,460 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-22 18:51:40,857 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-22 18:51:41,003 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-22 18:51:41,003 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-22 18:51:41,006 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2017-03-22 18:51:41,006 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2017-03-22 18:51:41,420 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-22 18:51:41,516 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-22 18:51:41,525 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-22 18:51:41,545 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-22 18:51:41,551 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-22 18:51:41,556 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-22 18:51:41,556 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-22 18:51:41,556 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-22 18:51:41,710 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-22 18:51:41,711 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-22 18:51:41,759 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-22 18:51:41,759 INFO org.mortbay.log: jetty-6.1.26
2017-03-22 18:51:42,045 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-22 18:51:42,075 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 18:51:42,075 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 18:51:42,116 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-22 18:51:42,116 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-22 18:51:42,156 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-22 18:51:42,156 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-22 18:51:42,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-22 18:51:42,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 22 18:51:42
2017-03-22 18:51:42,158 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-22 18:51:42,158 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 18:51:42,169 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-22 18:51:42,169 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-22 18:51:42,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-22 18:51:42,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-22 18:51:42,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-22 18:51:42,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-22 18:51:42,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-22 18:51:42,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-22 18:51:42,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-22 18:51:42,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-22 18:51:42,181 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-22 18:51:42,181 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-22 18:51:42,181 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-22 18:51:42,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-22 18:51:42,183 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-22 18:51:42,308 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-22 18:51:42,309 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 18:51:42,309 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-22 18:51:42,309 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-22 18:51:42,310 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-22 18:51:42,310 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-22 18:51:42,310 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-22 18:51:42,310 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-22 18:51:42,316 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-22 18:51:42,316 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 18:51:42,316 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-22 18:51:42,317 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-22 18:51:42,318 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-22 18:51:42,318 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-22 18:51:42,318 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-22 18:51:42,321 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-22 18:51:42,321 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-22 18:51:42,321 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-22 18:51:42,322 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-22 18:51:42,322 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-22 18:51:42,324 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-22 18:51:42,324 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 18:51:42,324 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-22 18:51:42,324 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-22 18:51:42,328 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode does not exist
2017-03-22 18:51:42,329 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:327)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:215)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:975)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:585)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:645)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:812)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:796)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1493)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1559)
2017-03-22 18:51:42,332 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-22 18:51:42,432 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2017-03-22 18:51:42,432 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2017-03-22 18:51:42,433 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2017-03-22 18:51:42,433 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:327)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:215)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:975)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:681)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:585)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:645)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:812)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:796)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1493)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1559)
2017-03-22 18:51:42,434 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2017-03-22 18:51:42,435 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-22 19:16:16,217 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-22 19:16:16,226 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-22 19:16:16,230 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-22 19:16:16,561 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-22 19:16:16,650 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-22 19:16:16,651 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-22 19:16:16,653 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://127.0.0.1:9000
2017-03-22 19:16:16,654 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 127.0.0.1:9000 to access this namenode/service.
2017-03-22 19:16:16,955 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-22 19:16:17,026 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-22 19:16:17,037 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-22 19:16:17,043 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-22 19:16:17,050 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-22 19:16:17,054 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-22 19:16:17,055 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-22 19:16:17,055 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-22 19:16:17,241 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-22 19:16:17,242 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-22 19:16:17,258 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-22 19:16:17,259 INFO org.mortbay.log: jetty-6.1.26
2017-03-22 19:16:17,457 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-22 19:16:17,494 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 19:16:17,494 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-22 19:16:17,535 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-22 19:16:17,535 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-22 19:16:17,575 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-22 19:16:17,575 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-22 19:16:17,576 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-22 19:16:17,577 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 22 19:16:17
2017-03-22 19:16:17,578 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-22 19:16:17,578 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 19:16:17,581 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-22 19:16:17,581 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-22 19:16:17,588 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-22 19:16:17,588 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-22 19:16:17,588 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-22 19:16:17,588 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-22 19:16:17,588 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-22 19:16:17,588 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-22 19:16:17,588 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-22 19:16:17,588 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-22 19:16:17,594 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-22 19:16:17,594 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-22 19:16:17,594 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-22 19:16:17,594 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-22 19:16:17,596 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-22 19:16:17,634 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-22 19:16:17,634 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 19:16:17,634 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-22 19:16:17,634 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-22 19:16:17,636 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-22 19:16:17,636 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-22 19:16:17,636 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-22 19:16:17,636 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-22 19:16:17,642 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-22 19:16:17,642 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 19:16:17,642 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-22 19:16:17,642 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-22 19:16:17,644 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-22 19:16:17,644 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-22 19:16:17,644 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-22 19:16:17,646 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-22 19:16:17,646 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-22 19:16:17,646 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-22 19:16:17,647 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-22 19:16:17,647 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-22 19:16:17,649 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-22 19:16:17,649 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-22 19:16:17,649 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-22 19:16:17,649 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-22 19:16:17,734 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/in_use.lock acquired by nodename 24384@gaurav-Inspiron-3542
2017-03-22 19:16:17,811 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current
2017-03-22 19:16:17,812 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2017-03-22 19:16:17,812 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-03-22 19:16:17,862 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-03-22 19:16:17,939 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-22 19:16:17,939 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000000
2017-03-22 19:16:17,992 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-03-22 19:16:17,992 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2017-03-22 19:16:18,146 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-22 19:16:18,146 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 493 msecs
2017-03-22 19:16:18,289 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2017-03-22 19:16:18,295 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-22 19:16:18,304 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-03-22 19:16:18,353 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-22 19:16:18,407 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-22 19:16:18,407 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-22 19:16:18,407 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-22 19:16:18,407 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2017-03-22 19:16:18,408 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-03-22 19:16:18,408 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-22 19:16:18,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 19:16:18,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-03-22 19:16:18,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-22 19:16:18,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-22 19:16:18,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-22 19:16:18,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-22 19:16:18,418 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 10 msec
2017-03-22 19:16:18,447 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-22 19:16:18,447 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-03-22 19:16:18,451 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2017-03-22 19:16:18,451 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-22 19:16:18,455 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-22 19:16:29,440 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0) storage 94c459fd-27e9-4f2a-9627-5d1c3b12d5ea
2017-03-22 19:16:29,440 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 19:16:29,441 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2017-03-22 19:16:29,538 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-22 19:16:29,538 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d832d0b9-b0fc-4e15-b505-a192e216a448 for DN 127.0.0.1:50010
2017-03-22 19:16:29,576 INFO BlockStateChange: BLOCK* processReport: from storage DS-d832d0b9-b0fc-4e15-b505-a192e216a448 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2017-03-22 19:17:37,539 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-03-22 19:17:37,539 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-22 19:17:37,539 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2017-03-22 19:17:37,539 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 7 Total time for transactions(ms): 17 Number of transactions batched in Syncs: 0 Number of syncs: 6 SyncTimes(ms): 99 
2017-03-22 19:17:37,556 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 7 Total time for transactions(ms): 17 Number of transactions batched in Syncs: 0 Number of syncs: 7 SyncTimes(ms): 117 
2017-03-22 19:17:37,557 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000000001 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000001-0000000000000000007
2017-03-22 19:17:37,560 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 8
2017-03-22 19:17:39,113 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2017-03-22 19:17:39,113 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000007 size 724 bytes.
2017-03-22 19:17:39,147 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2017-03-22 19:41:28,238 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 77 
2017-03-22 19:43:30,161 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 6 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 96 
2017-03-22 19:43:30,238 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/fdv.csv
2017-03-22 19:43:30,524 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/lungs/fdv.csv
2017-03-22 19:43:30,524 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2017-03-22 19:43:30,562 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 402894
2017-03-22 19:43:30,943 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/fdv.csv is closed by DFSClient_NONMAPREDUCE_-148602120_1
2017-03-22 19:44:08,423 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741825_1001 127.0.0.1:50010 
2017-03-22 19:44:09,583 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741825_1001]
2017-03-22 19:44:56,253 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 20 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 1 Number of syncs: 15 SyncTimes(ms): 262 
2017-03-22 19:45:02,719 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/fdv.csv
2017-03-22 19:45:02,734 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-22 19:45:02,738 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/fdv.csv is closed by DFSClient_NONMAPREDUCE_-148602120_1
2017-03-22 20:05:15,987 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 34 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 2 Number of syncs: 26 SyncTimes(ms): 355 
2017-03-22 20:17:39,660 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-03-22 20:17:39,660 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-22 20:17:39,660 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 8
2017-03-22 20:17:39,661 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 35 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 3 Number of syncs: 27 SyncTimes(ms): 407 
2017-03-22 20:17:39,680 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 35 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 3 Number of syncs: 28 SyncTimes(ms): 426 
2017-03-22 20:17:39,681 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000000008 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000008-0000000000000000042
2017-03-22 20:17:39,688 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 43
2017-03-22 20:17:40,623 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2017-03-22 20:17:40,623 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000042 size 883 bytes.
2017-03-22 20:17:40,664 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 7
2017-03-22 20:17:40,665 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-03-22 20:27:27,969 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 95 
2017-03-22 20:29:43,887 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 152 
2017-03-22 20:29:43,918 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741826_1002 127.0.0.1:50010 
2017-03-22 20:29:45,898 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741826_1002]
2017-03-22 20:31:05,455 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 6 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 6 SyncTimes(ms): 200 
2017-03-22 20:31:05,494 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/fdv.csv
2017-03-22 20:31:05,664 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-22 20:31:05,679 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/fdv.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-22 20:35:10,660 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 19 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 1 Number of syncs: 16 SyncTimes(ms): 356 
2017-03-22 20:35:10,672 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741827_1003 127.0.0.1:50010 
2017-03-22 20:35:12,930 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741827_1003]
2017-03-22 20:37:35,112 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 20 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 1 Number of syncs: 17 SyncTimes(ms): 368 
2017-03-22 20:38:03,930 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/dummy.txt
2017-03-22 20:38:03,947 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-22 20:38:03,960 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/dummy.txt is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-22 20:38:55,574 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 34 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 2 Number of syncs: 28 SyncTimes(ms): 485 
2017-03-22 20:38:55,586 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741828_1004 127.0.0.1:50010 
2017-03-22 20:38:57,953 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741828_1004]
2017-03-22 20:42:23,354 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 35 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 2 Number of syncs: 29 SyncTimes(ms): 497 
2017-03-22 20:42:29,969 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/dummy.txt
2017-03-22 20:42:29,983 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-22 20:42:29,988 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/dummy.txt is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-22 22:42:40,508 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1985ms
No GCs detected
2017-03-22 22:45:46,392 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 49 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 3 Number of syncs: 40 SyncTimes(ms): 586 
2017-03-22 22:45:46,409 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741829_1005 127.0.0.1:50010 
2017-03-22 22:45:48,556 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741829_1005]
2017-03-22 22:47:02,842 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 51 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 3 Number of syncs: 42 SyncTimes(ms): 614 
2017-03-22 22:47:02,901 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/dummy.txt
2017-03-22 22:47:02,919 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-22 22:47:02,931 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/dummy.txt is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-22 22:48:50,626 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 64 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 4 Number of syncs: 52 SyncTimes(ms): 867 
2017-03-22 22:48:50,639 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741830_1006 127.0.0.1:50010 
2017-03-22 22:48:51,571 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741830_1006]
2017-03-22 22:52:20,277 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 65 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 4 Number of syncs: 53 SyncTimes(ms): 880 
2017-03-22 22:52:30,578 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/dummy.txt
2017-03-22 22:52:30,594 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-22 22:52:30,598 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/dummy.txt is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-22 22:57:40,306 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 79 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 5 Number of syncs: 64 SyncTimes(ms): 1027 
2017-03-22 22:57:40,350 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741831_1007 127.0.0.1:50010 
2017-03-22 22:57:42,608 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741831_1007]
2017-03-22 22:58:21,193 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/dummy.txt
2017-03-22 22:58:21,208 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-22 22:58:21,212 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/dummy.txt is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-22 23:17:20,077 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-03-22 23:17:20,077 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-22 23:17:20,078 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 43
2017-03-22 23:17:20,078 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 94 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 6 Number of syncs: 76 SyncTimes(ms): 1206 
2017-03-22 23:17:20,103 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 94 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 6 Number of syncs: 77 SyncTimes(ms): 1231 
2017-03-22 23:17:20,104 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000000043 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000043-0000000000000000136
2017-03-22 23:17:20,117 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 137
2017-03-22 23:17:20,468 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 23.26 KB/s
2017-03-22 23:17:20,468 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000136 size 1053 bytes.
2017-03-22 23:17:20,515 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 42
2017-03-22 23:17:20,515 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000007, cpktTxId=0000000000000000007)
2017-03-22 23:32:07,878 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 81 
2017-03-22 23:32:07,886 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741832_1008 127.0.0.1:50010 
2017-03-22 23:32:09,767 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741832_1008]
2017-03-22 23:33:15,764 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 106 
2017-03-22 23:33:15,793 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/fdv.csv
2017-03-22 23:33:15,807 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-22 23:33:15,812 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/fdv.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:11:24,371 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1840ms
No GCs detected
2017-03-23 00:40:40,286 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 17 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 1 Number of syncs: 14 SyncTimes(ms): 229 
2017-03-23 00:40:40,292 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741833_1009 127.0.0.1:50010 
2017-03-23 00:40:42,509 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741833_1009]
2017-03-23 00:43:53,214 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-03-23 00:43:53,214 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-23 00:43:53,214 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 137
2017-03-23 00:43:53,214 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 19 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 1 Number of syncs: 16 SyncTimes(ms): 260 
2017-03-23 00:43:53,237 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 19 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 1 Number of syncs: 17 SyncTimes(ms): 283 
2017-03-23 00:43:53,250 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000000137 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000137-0000000000000000155
2017-03-23 00:43:53,267 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 156
2017-03-23 00:43:53,846 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.10s at 0.00 KB/s
2017-03-23 00:43:53,846 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000155 size 976 bytes.
2017-03-23 00:43:53,931 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 136
2017-03-23 00:43:53,931 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000042, cpktTxId=0000000000000000042)
2017-03-23 00:47:02,499 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 83 
2017-03-23 00:47:02,568 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/0.csv
2017-03-23 00:47:02,743 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:47:02,761 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/0.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:47:20,773 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741834_1010 127.0.0.1:50010 
2017-03-23 00:47:21,540 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741834_1010]
2017-03-23 00:47:38,743 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/0.csv
2017-03-23 00:47:38,757 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:47:38,763 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/0.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:48:36,470 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 30 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 2 Number of syncs: 24 SyncTimes(ms): 436 
2017-03-23 00:48:36,478 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741835_1011 127.0.0.1:50010 
2017-03-23 00:48:36,546 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741835_1011]
2017-03-23 00:48:42,792 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/0.csv
2017-03-23 00:48:42,840 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:48:42,867 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/0.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:49:00,435 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741836_1012 127.0.0.1:50010 
2017-03-23 00:49:00,548 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741836_1012]
2017-03-23 00:49:28,739 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/0.csv
2017-03-23 00:49:28,752 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:49:28,758 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/0.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:50:18,103 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 60 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 4 Number of syncs: 48 SyncTimes(ms): 774 
2017-03-23 00:50:18,117 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741837_1013 127.0.0.1:50010 
2017-03-23 00:50:18,555 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741837_1013]
2017-03-23 00:51:21,873 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 63 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 4 Number of syncs: 51 SyncTimes(ms): 810 
2017-03-23 00:53:25,072 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 64 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 4 Number of syncs: 52 SyncTimes(ms): 836 
2017-03-23 00:53:25,109 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/0.csv
2017-03-23 00:53:25,213 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:53:25,251 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/0.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:53:44,511 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/1.csv
2017-03-23 00:53:44,528 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:53:44,541 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/1.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:53:57,978 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/2.csv
2017-03-23 00:53:57,995 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:53:58,008 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/2.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:54:03,524 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/3.csv
2017-03-23 00:54:03,538 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:54:03,597 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/3.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:54:08,023 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/4.csv
2017-03-23 00:54:08,054 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:54:08,075 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/4.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:54:15,402 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/5.csv
2017-03-23 00:54:15,450 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:54:15,454 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/5.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:54:18,880 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/6.csv
2017-03-23 00:54:18,895 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:54:18,898 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/6.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:54:22,702 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/7.csv
2017-03-23 00:54:22,716 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:54:22,721 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/7.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:54:27,001 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 126 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 6 Number of syncs: 96 SyncTimes(ms): 1483 
2017-03-23 00:54:27,046 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/8.csv
2017-03-23 00:54:27,095 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:54:27,132 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/8.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:00,070 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/9.csv
2017-03-23 00:55:00,102 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:00,134 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/9.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:04,757 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/10.csv
2017-03-23 00:55:04,796 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:04,801 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/10.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:08,237 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/11.csv
2017-03-23 00:55:08,251 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:08,257 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/11.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:14,149 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/12.csv
2017-03-23 00:55:14,185 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:14,224 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/12.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:19,306 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/13.csv
2017-03-23 00:55:19,320 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:19,324 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/13.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:23,250 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/14.csv
2017-03-23 00:55:23,264 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:23,269 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/14.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:26,494 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/15.csv
2017-03-23 00:55:26,551 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:26,569 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/15.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:29,640 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 181 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 7 Number of syncs: 134 SyncTimes(ms): 2113 
2017-03-23 00:55:29,662 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/16.csv
2017-03-23 00:55:29,711 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:29,725 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/16.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:33,384 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/17.csv
2017-03-23 00:55:33,397 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:33,403 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/17.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:37,040 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/18.csv
2017-03-23 00:55:37,055 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:37,059 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/18.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:41,273 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/19.csv
2017-03-23 00:55:41,287 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:41,303 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/19.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:55:45,718 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/20.csv
2017-03-23 00:55:45,731 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:55:45,737 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/20.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:56:05,863 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/21.csv
2017-03-23 00:56:05,877 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:56:05,883 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/21.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:56:10,241 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/22.csv
2017-03-23 00:56:10,259 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:56:10,272 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/22.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:56:16,230 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/23.csv
2017-03-23 00:56:16,279 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:56:16,317 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/23.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:56:20,464 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/24.csv
2017-03-23 00:56:20,478 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:56:20,483 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/24.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:56:31,369 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 235 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 7 Number of syncs: 170 SyncTimes(ms): 2601 
2017-03-23 00:56:31,408 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/25.csv
2017-03-23 00:56:31,446 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:56:31,451 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/25.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:56:36,054 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/26.csv
2017-03-23 00:56:36,068 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:56:36,073 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/26.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:56:39,432 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/27.csv
2017-03-23 00:56:39,446 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:56:39,451 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/27.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:56:42,366 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/28.csv
2017-03-23 00:56:42,380 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:56:42,385 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/28.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:56:45,655 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/29.csv
2017-03-23 00:56:45,702 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:56:45,707 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/29.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:56:50,400 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/30.csv
2017-03-23 00:56:50,413 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-23 00:56:50,452 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/30.csv is closed by DFSClient_NONMAPREDUCE_636407382_1
2017-03-23 00:57:37,006 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 276 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 7 Number of syncs: 198 SyncTimes(ms): 2997 
2017-03-23 01:29:42,653 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 278 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 8 Number of syncs: 200 SyncTimes(ms): 3049 
2017-03-23 01:30:06,165 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-23 01:30:08,389 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-23 14:22:07,568 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-23 14:22:07,588 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-23 14:22:07,602 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-23 14:22:08,279 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-23 14:22:08,545 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-23 14:22:08,545 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-23 14:22:08,550 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://127.0.0.1:9000
2017-03-23 14:22:08,550 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 127.0.0.1:9000 to access this namenode/service.
2017-03-23 14:22:09,204 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-23 14:22:09,439 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-23 14:22:09,499 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-23 14:22:09,537 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-23 14:22:09,548 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-23 14:22:09,554 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-23 14:22:09,554 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-23 14:22:09,554 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-23 14:22:09,926 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-23 14:22:09,932 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-23 14:22:10,013 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-23 14:22:10,013 INFO org.mortbay.log: jetty-6.1.26
2017-03-23 14:22:10,500 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-23 14:22:10,655 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-23 14:22:10,655 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-23 14:22:10,737 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-23 14:22:10,737 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-23 14:22:10,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-23 14:22:10,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-23 14:22:10,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-23 14:22:10,869 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 23 14:22:10
2017-03-23 14:22:10,871 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-23 14:22:10,871 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-23 14:22:10,901 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-23 14:22:10,901 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-23 14:22:10,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-23 14:22:10,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-23 14:22:10,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-23 14:22:10,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-23 14:22:10,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-23 14:22:10,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-23 14:22:10,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-23 14:22:10,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-23 14:22:10,937 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-23 14:22:10,937 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-23 14:22:10,937 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-23 14:22:10,937 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-23 14:22:10,942 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-23 14:22:11,139 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-23 14:22:11,139 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-23 14:22:11,139 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-23 14:22:11,139 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-23 14:22:11,140 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-23 14:22:11,140 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-23 14:22:11,140 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-23 14:22:11,141 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-23 14:22:11,161 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-23 14:22:11,161 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-23 14:22:11,162 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-23 14:22:11,162 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-23 14:22:11,164 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-23 14:22:11,164 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-23 14:22:11,164 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-23 14:22:11,180 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-23 14:22:11,180 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-23 14:22:11,180 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-23 14:22:11,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-23 14:22:11,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-23 14:22:11,197 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-23 14:22:11,197 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-23 14:22:11,197 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-23 14:22:11,197 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-23 14:22:11,285 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/in_use.lock acquired by nodename 4260@gaurav-Inspiron-3542
2017-03-23 14:22:11,476 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current
2017-03-23 14:22:11,899 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000000156 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000156-0000000000000000433
2017-03-23 14:22:11,916 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000155, cpktTxId=0000000000000000155)
2017-03-23 14:22:12,031 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 10 INodes.
2017-03-23 14:22:12,079 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-23 14:22:12,087 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 155 from /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000155
2017-03-23 14:22:12,087 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2a34281f expecting start txid #156
2017-03-23 14:22:12,088 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000156-0000000000000000433
2017-03-23 14:22:12,090 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000156-0000000000000000433' to transaction ID 156
2017-03-23 14:22:12,244 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000156-0000000000000000433 of size 1048576 edits # 278 loaded in 0 seconds
2017-03-23 14:22:12,244 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2017-03-23 14:22:12,244 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2017-03-23 14:22:12,252 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage.ckpt_0000000000000000433 using no compression
2017-03-23 14:22:12,319 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage.ckpt_0000000000000000433 of size 2884 bytes saved in 0 seconds.
2017-03-23 14:22:12,355 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 155
2017-03-23 14:22:12,355 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000136, cpktTxId=0000000000000000136)
2017-03-23 14:22:12,432 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 434
2017-03-23 14:22:12,604 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-23 14:22:12,604 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1389 msecs
2017-03-23 14:22:13,002 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2017-03-23 14:22:13,022 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-23 14:22:13,036 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-03-23 14:22:13,269 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-23 14:22:13,304 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-23 14:22:13,304 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-23 14:22:13,305 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 31 blocks to reach the threshold 0.9990 of total blocks 31.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-03-23 14:22:13,318 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-23 14:22:13,435 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-23 14:22:13,435 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-03-23 14:22:13,484 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2017-03-23 14:22:13,484 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-23 14:22:13,547 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-23 14:22:20,979 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0) storage 94c459fd-27e9-4f2a-9627-5d1c3b12d5ea
2017-03-23 14:22:20,987 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-23 14:22:20,989 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2017-03-23 14:22:21,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-23 14:22:21,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d832d0b9-b0fc-4e15-b505-a192e216a448 for DN 127.0.0.1:50010
2017-03-23 14:22:21,328 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 30 has reached the threshold 0.9990 of total blocks 31. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2017-03-23 14:22:21,328 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-23 14:22:21,329 INFO BlockStateChange: BLOCK* processReport: from storage DS-d832d0b9-b0fc-4e15-b505-a192e216a448 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0), blocks: 31, hasStaleStorage: false, processing time: 24 msecs
2017-03-23 14:22:21,358 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 31
2017-03-23 14:22:21,358 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-23 14:22:21,358 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-23 14:22:21,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-23 14:22:21,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-23 14:22:21,359 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 31 msec
2017-03-23 14:22:41,332 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 31 has reached the threshold 0.9990 of total blocks 31. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2017-03-23 14:22:51,333 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 40 secs
2017-03-23 14:22:51,333 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2017-03-23 14:22:51,333 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2017-03-23 14:22:51,333 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-23 14:23:59,285 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 76 
2017-03-23 14:25:16,407 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 40 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 0 Number of syncs: 8 SyncTimes(ms): 176 
2017-03-23 14:26:45,813 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 6899ms
No GCs detected
2017-03-23 14:51:59,883 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 42 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 1 Number of syncs: 10 SyncTimes(ms): 201 
2017-03-23 14:52:41,508 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-23 14:52:41,835 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-24 18:51:17,304 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-24 18:51:17,325 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-24 18:51:17,331 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-24 18:51:17,809 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-24 18:51:17,960 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-24 18:51:17,960 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-24 18:51:17,963 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://127.0.0.1:9000
2017-03-24 18:51:17,963 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 127.0.0.1:9000 to access this namenode/service.
2017-03-24 18:51:18,515 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-24 18:51:18,663 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-24 18:51:18,689 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-24 18:51:18,702 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-24 18:51:18,707 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-24 18:51:18,711 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-24 18:51:18,711 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-24 18:51:18,711 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-24 18:51:18,962 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-24 18:51:18,969 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-24 18:51:19,061 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-24 18:51:19,061 INFO org.mortbay.log: jetty-6.1.26
2017-03-24 18:51:19,402 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-24 18:51:19,542 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-24 18:51:19,542 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-24 18:51:19,595 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-24 18:51:19,595 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-24 18:51:19,702 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-24 18:51:19,702 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-24 18:51:19,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-24 18:51:19,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 24 18:51:19
2017-03-24 18:51:19,704 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-24 18:51:19,704 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 18:51:19,714 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-24 18:51:19,714 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-24 18:51:19,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-24 18:51:19,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-24 18:51:19,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-24 18:51:19,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-24 18:51:19,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-24 18:51:19,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-24 18:51:19,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-24 18:51:19,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-24 18:51:19,732 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-24 18:51:19,732 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-24 18:51:19,732 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-24 18:51:19,732 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-24 18:51:19,734 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-24 18:51:19,922 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-24 18:51:19,923 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 18:51:19,923 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-24 18:51:19,923 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-24 18:51:19,924 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-24 18:51:19,924 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-24 18:51:19,924 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-24 18:51:19,924 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-24 18:51:19,942 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-24 18:51:19,942 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 18:51:19,943 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-24 18:51:19,943 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-24 18:51:19,944 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-24 18:51:19,944 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-24 18:51:19,944 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-24 18:51:19,961 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-24 18:51:19,961 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-24 18:51:19,961 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-24 18:51:19,963 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-24 18:51:19,963 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-24 18:51:19,977 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-24 18:51:19,978 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 18:51:19,978 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-24 18:51:19,978 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-24 18:51:20,062 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/in_use.lock acquired by nodename 3794@gaurav-Inspiron-3542
2017-03-24 18:51:20,226 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current
2017-03-24 18:51:20,458 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000000434 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000434-0000000000000000475
2017-03-24 18:51:20,493 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000433, cpktTxId=0000000000000000433)
2017-03-24 18:51:20,588 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 39 INodes.
2017-03-24 18:51:20,646 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-24 18:51:20,647 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 433 from /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000433
2017-03-24 18:51:20,647 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@40e16993 expecting start txid #434
2017-03-24 18:51:20,647 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000434-0000000000000000475
2017-03-24 18:51:20,648 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000434-0000000000000000475' to transaction ID 434
2017-03-24 18:51:20,722 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000434-0000000000000000475 of size 1048576 edits # 42 loaded in 0 seconds
2017-03-24 18:51:20,723 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2017-03-24 18:51:20,723 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2017-03-24 18:51:20,728 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage.ckpt_0000000000000000475 using no compression
2017-03-24 18:51:20,763 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage.ckpt_0000000000000000475 of size 2884 bytes saved in 0 seconds.
2017-03-24 18:51:20,798 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 433
2017-03-24 18:51:20,799 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000155, cpktTxId=0000000000000000155)
2017-03-24 18:51:20,865 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 476
2017-03-24 18:51:21,042 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-24 18:51:21,042 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1046 msecs
2017-03-24 18:51:21,374 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2017-03-24 18:51:21,391 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-24 18:51:21,400 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-03-24 18:51:21,560 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-24 18:51:21,567 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-24 18:51:21,567 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-24 18:51:21,568 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 31 blocks to reach the threshold 0.9990 of total blocks 31.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-03-24 18:51:21,575 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-24 18:51:21,650 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-24 18:51:21,652 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-03-24 18:51:21,671 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2017-03-24 18:51:21,671 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-24 18:51:21,678 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-24 18:51:27,657 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0) storage 94c459fd-27e9-4f2a-9627-5d1c3b12d5ea
2017-03-24 18:51:27,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-24 18:51:27,657 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2017-03-24 18:51:27,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-24 18:51:27,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d832d0b9-b0fc-4e15-b505-a192e216a448 for DN 127.0.0.1:50010
2017-03-24 18:51:27,792 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 30 has reached the threshold 0.9990 of total blocks 31. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2017-03-24 18:51:27,792 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-24 18:51:27,794 INFO BlockStateChange: BLOCK* processReport: from storage DS-d832d0b9-b0fc-4e15-b505-a192e216a448 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0), blocks: 31, hasStaleStorage: false, processing time: 8 msecs
2017-03-24 18:51:27,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 31
2017-03-24 18:51:27,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-24 18:51:27,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-24 18:51:27,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-24 18:51:27,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-24 18:51:27,796 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 4 msec
2017-03-24 18:51:47,797 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 31 has reached the threshold 0.9990 of total blocks 31. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2017-03-24 18:51:57,799 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 38 secs
2017-03-24 18:51:57,799 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2017-03-24 18:51:57,799 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2017-03-24 18:51:57,799 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-24 18:53:15,981 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 142 
2017-03-24 18:53:32,379 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/30_copy_1.csv
2017-03-24 18:53:32,680 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741869_1045{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/lungs/30_copy_1.csv
2017-03-24 18:53:32,680 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2017-03-24 18:53:32,698 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741869_1045{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 76
2017-03-24 18:53:33,106 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/30_copy_1.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 18:54:09,234 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/lungs/28_copy_1.csv
2017-03-24 18:54:09,248 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 18:54:09,252 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/lungs/28_copy_1.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 18:54:21,997 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 31 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 2 Number of syncs: 26 SyncTimes(ms): 359 
2017-03-24 18:54:43,047 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/28.csv
2017-03-24 18:54:43,060 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 18:54:43,065 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/28.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 18:59:52,433 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 51 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 4 Number of syncs: 42 SyncTimes(ms): 544 
2017-03-24 18:59:52,443 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/28_copy_1.csv
2017-03-24 18:59:52,457 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 18:59:52,462 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/28_copy_1.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 19:10:30,777 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 64 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 5 Number of syncs: 52 SyncTimes(ms): 663 
2017-03-24 19:10:30,794 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/28_copy_2.csv
2017-03-24 19:10:30,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 19:10:30,813 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/28_copy_2.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 19:11:34,967 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 84 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 7 Number of syncs: 68 SyncTimes(ms): 811 
2017-03-24 19:11:34,987 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/24.csv
2017-03-24 19:11:35,001 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 19:11:35,006 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/24.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 19:12:01,386 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741874_1050 127.0.0.1:50010 
2017-03-24 19:12:01,386 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741871_1047 127.0.0.1:50010 
2017-03-24 19:12:01,386 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741872_1048 127.0.0.1:50010 
2017-03-24 19:12:01,386 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741873_1049 127.0.0.1:50010 
2017-03-24 19:12:03,689 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741872_1048, blk_1073741873_1049, blk_1073741874_1050, blk_1073741871_1047]
2017-03-24 19:12:22,590 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/24.csv
2017-03-24 19:12:22,604 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 19:12:22,609 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/24.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 19:13:16,904 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 119 Total time for transactions(ms): 16 Number of transactions batched in Syncs: 10 Number of syncs: 96 SyncTimes(ms): 1150 
2017-03-24 19:14:19,357 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 164 Total time for transactions(ms): 18 Number of transactions batched in Syncs: 12 Number of syncs: 108 SyncTimes(ms): 1304 
2017-03-24 19:20:54,503 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 185 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 15 Number of syncs: 126 SyncTimes(ms): 1522 
2017-03-24 19:20:54,540 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741875_1051 127.0.0.1:50010 
2017-03-24 19:20:54,739 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741875_1051]
2017-03-24 19:21:23,112 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/24.csv
2017-03-24 19:21:23,126 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 19:21:23,131 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/24.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 19:23:55,428 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 207 Total time for transactions(ms): 24 Number of transactions batched in Syncs: 17 Number of syncs: 144 SyncTimes(ms): 1741 
2017-03-24 19:23:55,441 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741876_1052 127.0.0.1:50010 
2017-03-24 19:23:57,758 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741876_1052]
2017-03-24 19:24:06,967 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/24.csv
2017-03-24 19:24:07,002 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 19:24:07,008 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/24.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 19:24:39,443 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741877_1053 127.0.0.1:50010 
2017-03-24 19:24:39,762 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741877_1053]
2017-03-24 19:24:58,005 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 223 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 18 Number of syncs: 157 SyncTimes(ms): 1911 
2017-03-24 19:25:01,636 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/24.csv
2017-03-24 19:25:01,651 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 19:25:01,656 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/24.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 19:25:09,645 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741878_1054 127.0.0.1:50010 
2017-03-24 19:25:09,765 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741878_1054]
2017-03-24 19:25:16,226 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/24.csv
2017-03-24 19:25:16,240 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 19:25:16,256 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/24.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 19:27:40,700 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-03-24 19:27:40,700 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-24 19:27:40,700 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 476
2017-03-24 19:27:40,700 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 252 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 20 Number of syncs: 180 SyncTimes(ms): 2219 
2017-03-24 19:27:40,721 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 252 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 20 Number of syncs: 181 SyncTimes(ms): 2240 
2017-03-24 19:27:40,736 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000000476 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000476-0000000000000000727
2017-03-24 19:27:40,752 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 728
2017-03-24 19:27:44,464 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 58.82 KB/s
2017-03-24 19:27:44,464 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000727 size 3326 bytes.
2017-03-24 19:27:44,506 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 475
2017-03-24 19:27:44,506 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000433, cpktTxId=0000000000000000433)
2017-03-24 19:31:39,636 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 89 
2017-03-24 19:31:39,646 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741879_1055 127.0.0.1:50010 
2017-03-24 19:31:39,801 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741879_1055]
2017-03-24 19:31:45,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/24.csv
2017-03-24 19:31:45,952 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 19:31:45,957 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/24.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 19:32:00,081 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741880_1056 127.0.0.1:50010 
2017-03-24 19:32:00,803 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741880_1056]
2017-03-24 19:32:08,850 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/24.csv
2017-03-24 19:32:08,865 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 19:32:08,870 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/24.csv is closed by DFSClient_NONMAPREDUCE_-41331195_1
2017-03-24 19:34:35,361 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 39 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 3 Number of syncs: 32 SyncTimes(ms): 450 
2017-03-24 20:46:39,668 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2047ms
No GCs detected
2017-03-24 20:47:53,118 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 40 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 3 Number of syncs: 33 SyncTimes(ms): 468 
2017-03-24 20:48:14,758 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741881_1057 127.0.0.1:50010 
2017-03-24 20:48:14,827 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741881_1057]
2017-03-24 21:14:20,548 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 43 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 3 Number of syncs: 36 SyncTimes(ms): 541 
2017-03-24 21:14:45,889 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/24.csv
2017-03-24 21:14:46,284 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 21:14:46,297 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/24.csv is closed by DFSClient_NONMAPREDUCE_-733899232_1
2017-03-24 21:15:19,688 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741882_1058 127.0.0.1:50010 
2017-03-24 21:15:20,985 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741882_1058]
2017-03-24 21:15:46,411 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 58 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 4 Number of syncs: 48 SyncTimes(ms): 724 
2017-03-24 21:15:53,971 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/24.csv
2017-03-24 21:15:53,986 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 21:15:53,990 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/24.csv is closed by DFSClient_NONMAPREDUCE_-733899232_1
2017-03-24 21:16:00,638 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/21.csv
2017-03-24 21:16:00,652 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 21:16:00,680 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/21.csv is closed by DFSClient_NONMAPREDUCE_-733899232_1
2017-03-24 21:19:05,522 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 78 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 5 Number of syncs: 63 SyncTimes(ms): 942 
2017-03-24 21:22:21,436 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 85 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 6 Number of syncs: 69 SyncTimes(ms): 987 
2017-03-24 21:22:21,461 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/hadoop/b8996e3e-8fbb-4ed9-9f80-309cb25d54da/_tmp_space.db/Values__Tmp__Table__1/data_file
2017-03-24 21:22:21,474 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 21:22:21,480 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/hadoop/b8996e3e-8fbb-4ed9-9f80-309cb25d54da/_tmp_space.db/Values__Tmp__Table__1/data_file is closed by DFSClient_NONMAPREDUCE_-733899232_1
2017-03-24 21:36:26,640 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 93 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 6 Number of syncs: 75 SyncTimes(ms): 1044 
2017-03-24 21:36:26,655 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741884_1060 127.0.0.1:50010 
2017-03-24 21:36:26,655 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741883_1059 127.0.0.1:50010 
2017-03-24 21:36:27,089 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741883_1059, blk_1073741884_1060]
2017-03-24 21:39:12,165 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 94 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 6 Number of syncs: 76 SyncTimes(ms): 1058 
2017-03-24 21:39:34,858 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-03-24 21:39:34,858 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-24 21:39:34,858 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 728
2017-03-24 21:39:34,877 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 95 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 6 Number of syncs: 78 SyncTimes(ms): 1110 
2017-03-24 21:39:34,878 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000000728 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000728-0000000000000000822
2017-03-24 21:39:34,887 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 823
2017-03-24 21:39:35,556 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 57.69 KB/s
2017-03-24 21:39:35,556 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000822 size 3409 bytes.
2017-03-24 21:39:35,607 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 727
2017-03-24 21:39:35,607 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000475, cpktTxId=0000000000000000475)
2017-03-24 21:41:02,141 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 358 
2017-03-24 21:41:02,164 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/hadoop/b8996e3e-8fbb-4ed9-9f80-309cb25d54da/_tmp_space.db/Values__Tmp__Table__2/data_file
2017-03-24 21:41:02,177 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 21:41:02,184 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/hadoop/b8996e3e-8fbb-4ed9-9f80-309cb25d54da/_tmp_space.db/Values__Tmp__Table__2/data_file is closed by DFSClient_NONMAPREDUCE_-733899232_1
2017-03-24 21:41:34,343 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hive/hadoop/b8996e3e-8fbb-4ed9-9f80-309cb25d54da/_tmp_space.db/Values__Tmp__Table__3/data_file
2017-03-24 21:41:34,357 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-24 21:41:34,363 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive/hadoop/b8996e3e-8fbb-4ed9-9f80-309cb25d54da/_tmp_space.db/Values__Tmp__Table__3/data_file is closed by DFSClient_NONMAPREDUCE_-733899232_1
2017-03-24 21:42:28,607 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 18 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 14 SyncTimes(ms): 447 
2017-03-24 21:44:43,328 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 19 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 15 SyncTimes(ms): 462 
2017-03-24 21:58:21,813 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 59 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 1 Number of syncs: 21 SyncTimes(ms): 511 
2017-03-24 21:58:21,888 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741885_1061 127.0.0.1:50010 
2017-03-24 21:58:21,889 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741886_1062 127.0.0.1:50010 
2017-03-24 21:58:21,889 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741887_1063 127.0.0.1:50010 
2017-03-24 21:58:24,188 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741885_1061, blk_1073741886_1062, blk_1073741887_1063]
2017-03-24 22:01:43,406 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-24 22:01:43,436 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-24 22:03:29,154 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-24 22:03:29,175 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-24 22:03:29,180 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-24 22:03:29,625 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-24 22:03:29,799 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-24 22:03:29,799 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-24 22:03:29,803 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://127.0.0.1:9000
2017-03-24 22:03:29,803 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 127.0.0.1:9000 to access this namenode/service.
2017-03-24 22:03:31,434 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-24 22:03:31,589 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-24 22:03:31,618 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-24 22:03:31,640 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-24 22:03:31,645 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-24 22:03:31,649 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-24 22:03:31,649 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-24 22:03:31,649 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-24 22:03:31,890 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-24 22:03:31,896 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-24 22:03:31,988 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-24 22:03:31,988 INFO org.mortbay.log: jetty-6.1.26
2017-03-24 22:03:32,307 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-24 22:03:32,459 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-24 22:03:32,459 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-24 22:03:32,507 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-24 22:03:32,507 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-24 22:03:32,606 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-24 22:03:32,606 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-24 22:03:32,607 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-24 22:03:32,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 24 22:03:32
2017-03-24 22:03:32,609 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-24 22:03:32,609 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 22:03:32,619 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-24 22:03:32,619 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-24 22:03:32,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-24 22:03:32,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-24 22:03:32,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-24 22:03:32,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-24 22:03:32,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-24 22:03:32,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-24 22:03:32,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-24 22:03:32,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-24 22:03:32,637 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-24 22:03:32,637 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-24 22:03:32,637 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-24 22:03:32,638 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-24 22:03:32,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-24 22:03:32,840 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-24 22:03:32,840 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 22:03:32,841 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-24 22:03:32,841 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-24 22:03:32,842 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-24 22:03:32,842 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-24 22:03:32,842 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-24 22:03:32,842 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-24 22:03:32,858 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-24 22:03:32,858 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 22:03:32,858 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-24 22:03:32,858 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-24 22:03:32,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-24 22:03:32,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-24 22:03:32,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-24 22:03:32,877 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-24 22:03:32,878 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-24 22:03:32,878 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-24 22:03:32,879 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-24 22:03:32,879 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-24 22:03:32,894 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-24 22:03:32,894 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 22:03:32,894 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-24 22:03:32,894 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-24 22:03:32,998 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/in_use.lock acquired by nodename 3075@gaurav-Inspiron-3542
2017-03-24 22:03:33,137 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current
2017-03-24 22:03:33,374 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000000823 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000823-0000000000000000881
2017-03-24 22:03:33,390 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000822, cpktTxId=0000000000000000822)
2017-03-24 22:03:33,504 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 46 INodes.
2017-03-24 22:03:33,563 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-24 22:03:33,563 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 822 from /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000822
2017-03-24 22:03:33,563 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@306acfc9 expecting start txid #823
2017-03-24 22:03:33,564 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000823-0000000000000000881
2017-03-24 22:03:33,565 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000823-0000000000000000881' to transaction ID 823
2017-03-24 22:03:33,653 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000823-0000000000000000881 of size 1048576 edits # 59 loaded in 0 seconds
2017-03-24 22:03:33,653 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-03-24 22:03:33,656 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 882
2017-03-24 22:03:33,788 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-24 22:03:33,788 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 876 msecs
2017-03-24 22:03:34,117 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2017-03-24 22:03:34,130 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-24 22:03:34,140 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-03-24 22:03:34,301 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-24 22:03:34,308 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-24 22:03:34,308 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-24 22:03:34,309 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 33 blocks to reach the threshold 0.9990 of total blocks 33.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-03-24 22:03:34,314 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-24 22:03:34,377 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-24 22:03:34,377 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-03-24 22:03:34,380 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2017-03-24 22:03:34,380 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-24 22:03:34,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-24 22:03:39,652 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0) storage 94c459fd-27e9-4f2a-9627-5d1c3b12d5ea
2017-03-24 22:03:39,653 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-24 22:03:39,653 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2017-03-24 22:03:39,772 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-24 22:03:39,772 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d832d0b9-b0fc-4e15-b505-a192e216a448 for DN 127.0.0.1:50010
2017-03-24 22:03:39,810 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 32 has reached the threshold 0.9990 of total blocks 33. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2017-03-24 22:03:39,810 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-24 22:03:39,811 INFO BlockStateChange: BLOCK* processReport: from storage DS-d832d0b9-b0fc-4e15-b505-a192e216a448 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0), blocks: 33, hasStaleStorage: false, processing time: 6 msecs
2017-03-24 22:03:39,822 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 33
2017-03-24 22:03:39,822 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-24 22:03:39,822 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-24 22:03:39,822 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-24 22:03:39,822 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-24 22:03:39,822 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 12 msec
2017-03-24 22:03:59,815 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 33 has reached the threshold 0.9990 of total blocks 33. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2017-03-24 22:04:09,817 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 37 secs
2017-03-24 22:04:09,817 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2017-03-24 22:04:09,817 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2017-03-24 22:04:09,817 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-24 22:04:51,403 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 68 
2017-03-24 22:06:59,768 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 132 
2017-03-24 22:07:46,375 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-24 22:07:46,377 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-24 23:52:13,059 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-24 23:52:13,077 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-24 23:52:13,082 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-24 23:52:13,555 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-24 23:52:13,704 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-24 23:52:13,704 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-24 23:52:13,707 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://127.0.0.1:9000
2017-03-24 23:52:13,707 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 127.0.0.1:9000 to access this namenode/service.
2017-03-24 23:52:14,248 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-24 23:52:14,394 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-24 23:52:14,421 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-24 23:52:14,434 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-24 23:52:14,440 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-24 23:52:14,443 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-24 23:52:14,444 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-24 23:52:14,444 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-24 23:52:14,683 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-24 23:52:14,690 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-24 23:52:14,782 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-24 23:52:14,782 INFO org.mortbay.log: jetty-6.1.26
2017-03-24 23:52:15,100 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-24 23:52:15,252 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-24 23:52:15,252 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-24 23:52:15,316 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-24 23:52:15,316 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-24 23:52:15,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-24 23:52:15,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-24 23:52:15,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-24 23:52:15,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 24 23:52:15
2017-03-24 23:52:15,415 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-24 23:52:15,415 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 23:52:15,424 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-24 23:52:15,424 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-24 23:52:15,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-24 23:52:15,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-24 23:52:15,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-24 23:52:15,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-24 23:52:15,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-24 23:52:15,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-24 23:52:15,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-24 23:52:15,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-24 23:52:15,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-24 23:52:15,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-24 23:52:15,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-24 23:52:15,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-24 23:52:15,444 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-24 23:52:15,654 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-24 23:52:15,654 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 23:52:15,654 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-24 23:52:15,654 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-24 23:52:15,656 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-24 23:52:15,656 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-24 23:52:15,656 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-24 23:52:15,656 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-24 23:52:15,674 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-24 23:52:15,674 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 23:52:15,674 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-24 23:52:15,674 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-24 23:52:15,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-24 23:52:15,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-24 23:52:15,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-24 23:52:15,693 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-24 23:52:15,693 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-24 23:52:15,693 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-24 23:52:15,694 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-24 23:52:15,694 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-24 23:52:15,710 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-24 23:52:15,710 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-24 23:52:15,710 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-24 23:52:15,710 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-24 23:52:15,794 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/in_use.lock acquired by nodename 3675@gaurav-Inspiron-3542
2017-03-24 23:52:15,955 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current
2017-03-24 23:52:16,199 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000000882 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000882-0000000000000000887
2017-03-24 23:52:16,239 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000822, cpktTxId=0000000000000000822)
2017-03-24 23:52:16,342 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 46 INodes.
2017-03-24 23:52:16,401 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-24 23:52:16,401 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 822 from /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000822
2017-03-24 23:52:16,401 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@10058911 expecting start txid #823
2017-03-24 23:52:16,401 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000823-0000000000000000881
2017-03-24 23:52:16,403 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000823-0000000000000000881' to transaction ID 823
2017-03-24 23:52:16,532 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000823-0000000000000000881 of size 1048576 edits # 59 loaded in 0 seconds
2017-03-24 23:52:16,532 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5b69c34e expecting start txid #882
2017-03-24 23:52:16,532 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000882-0000000000000000887
2017-03-24 23:52:16,532 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000882-0000000000000000887' to transaction ID 823
2017-03-24 23:52:16,535 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000882-0000000000000000887 of size 1048576 edits # 6 loaded in 0 seconds
2017-03-24 23:52:16,535 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2017-03-24 23:52:16,535 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2017-03-24 23:52:16,541 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage.ckpt_0000000000000000887 using no compression
2017-03-24 23:52:16,576 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage.ckpt_0000000000000000887 of size 3028 bytes saved in 0 seconds.
2017-03-24 23:52:16,608 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 822
2017-03-24 23:52:16,609 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000727, cpktTxId=0000000000000000727)
2017-03-24 23:52:16,677 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 888
2017-03-24 23:52:16,830 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-24 23:52:16,830 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1103 msecs
2017-03-24 23:52:17,168 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2017-03-24 23:52:17,179 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-24 23:52:17,188 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-03-24 23:52:17,348 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-24 23:52:17,354 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-24 23:52:17,355 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-24 23:52:17,355 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 33 blocks to reach the threshold 0.9990 of total blocks 33.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-03-24 23:52:17,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-24 23:52:17,424 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-24 23:52:17,425 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-03-24 23:52:17,442 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2017-03-24 23:52:17,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-24 23:52:17,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-24 23:52:23,102 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0) storage 94c459fd-27e9-4f2a-9627-5d1c3b12d5ea
2017-03-24 23:52:23,102 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-24 23:52:23,103 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2017-03-24 23:52:23,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-24 23:52:23,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d832d0b9-b0fc-4e15-b505-a192e216a448 for DN 127.0.0.1:50010
2017-03-24 23:52:23,256 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 32 has reached the threshold 0.9990 of total blocks 33. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2017-03-24 23:52:23,256 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-24 23:52:23,258 INFO BlockStateChange: BLOCK* processReport: from storage DS-d832d0b9-b0fc-4e15-b505-a192e216a448 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0), blocks: 33, hasStaleStorage: false, processing time: 8 msecs
2017-03-24 23:52:23,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 33
2017-03-24 23:52:23,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-24 23:52:23,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-24 23:52:23,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-24 23:52:23,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-24 23:52:23,261 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 4 msec
2017-03-24 23:52:43,262 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 33 has reached the threshold 0.9990 of total blocks 33. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2017-03-24 23:52:53,263 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 37 secs
2017-03-24 23:52:53,263 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2017-03-24 23:52:53,263 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2017-03-24 23:52:53,264 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-24 23:53:24,006 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 78 
2017-03-24 23:58:39,906 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 125 
2017-03-24 23:59:35,776 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=1/21.csv
2017-03-24 23:59:35,990 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741888_1064{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/patient/id=1/21.csv
2017-03-24 23:59:35,990 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2017-03-24 23:59:36,005 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741888_1064{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 433285
2017-03-24 23:59:36,428 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=1/21.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-24 23:59:45,163 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 12 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 11 SyncTimes(ms): 205 
2017-03-24 23:59:47,775 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2017-03-25 00:02:25,876 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 19 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 18 SyncTimes(ms): 299 
2017-03-25 00:02:25,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/21.csv
2017-03-25 00:02:25,939 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:02:25,949 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/21.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:06:06,084 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 33 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 1 Number of syncs: 29 SyncTimes(ms): 440 
2017-03-25 00:06:06,130 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741889_1065 127.0.0.1:50010 
2017-03-25 00:06:06,131 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741888_1064 127.0.0.1:50010 
2017-03-25 00:06:08,443 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741888_1064, blk_1073741889_1065]
2017-03-25 00:06:50,250 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/1.csv
2017-03-25 00:06:50,264 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:06:50,277 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/1.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:06:56,682 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/2.csv
2017-03-25 00:06:56,698 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:06:56,711 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/2.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:07:02,515 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/3.csv
2017-03-25 00:07:02,530 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:07:02,533 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/3.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:07:08,405 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 54 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 1 Number of syncs: 44 SyncTimes(ms): 640 
2017-03-25 00:07:08,427 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/4.csv
2017-03-25 00:07:08,441 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:07:08,456 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/4.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:07:14,749 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/5.csv
2017-03-25 00:07:14,763 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:07:14,767 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/5.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:07:20,938 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/6.csv
2017-03-25 00:07:20,952 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:07:20,957 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/6.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:07:26,361 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/7.csv
2017-03-25 00:07:26,375 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:07:26,379 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/7.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:07:31,917 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/8.csv
2017-03-25 00:07:31,932 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:07:31,935 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/8.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:07:37,728 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/9.csv
2017-03-25 00:07:37,742 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:07:37,747 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/9.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:07:44,351 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/10.csv
2017-03-25 00:07:44,365 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:07:44,369 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/10.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:08:15,981 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 103 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 2 Number of syncs: 78 SyncTimes(ms): 1091 
2017-03-25 00:08:15,997 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/11.csv
2017-03-25 00:08:16,012 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:08:16,016 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/11.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:08:20,953 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/12.csv
2017-03-25 00:08:21,008 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:08:21,016 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/12.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:08:26,776 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/13.csv
2017-03-25 00:08:26,790 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:08:26,794 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/13.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:08:31,843 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/14.csv
2017-03-25 00:08:31,858 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:08:31,872 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/14.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:08:36,521 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/15.csv
2017-03-25 00:08:36,537 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:08:36,551 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/15.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:08:42,753 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/16.csv
2017-03-25 00:08:42,790 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:08:42,795 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/16.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:08:48,877 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/17.csv
2017-03-25 00:08:48,894 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:08:48,907 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/17.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:08:54,945 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/18.csv
2017-03-25 00:08:54,960 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:08:54,974 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/18.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:09:00,022 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/19.csv
2017-03-25 00:09:00,038 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:09:00,041 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/19.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:09:05,694 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/20.csv
2017-03-25 00:09:05,727 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:09:05,730 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/20.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:09:11,022 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/21.csv
2017-03-25 00:09:11,038 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:09:11,042 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/21.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:09:31,281 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 169 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 2 Number of syncs: 122 SyncTimes(ms): 1649 
2017-03-25 00:09:31,303 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/22.csv
2017-03-25 00:09:31,317 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:09:31,321 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/22.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:09:40,258 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/23.csv
2017-03-25 00:09:40,272 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:09:40,277 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/23.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:09:51,603 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/24.csv
2017-03-25 00:09:51,617 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:09:51,622 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/24.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:10:00,426 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/25.csv
2017-03-25 00:10:00,440 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:10:00,445 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/25.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:10:18,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/26.csv
2017-03-25 00:10:18,986 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:10:18,990 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/26.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:10:24,639 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/27.csv
2017-03-25 00:10:24,653 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:10:24,657 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/27.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:10:29,717 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/28.csv
2017-03-25 00:10:29,735 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:10:29,747 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/28.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:10:34,695 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 211 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 2 Number of syncs: 150 SyncTimes(ms): 1956 
2017-03-25 00:10:34,718 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/29.csv
2017-03-25 00:10:34,735 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:10:34,747 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/29.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:10:42,573 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/30.csv
2017-03-25 00:10:42,590 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:10:42,603 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/30.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:10:48,863 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/31.csv
2017-03-25 00:10:48,914 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:10:48,926 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/31.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:42:44,160 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-03-25 00:42:44,202 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-03-25 00:42:44,202 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 888
2017-03-25 00:42:44,203 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 236 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 3 Number of syncs: 168 SyncTimes(ms): 2192 
2017-03-25 00:42:44,233 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 236 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 3 Number of syncs: 169 SyncTimes(ms): 2222 
2017-03-25 00:42:44,248 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000000888 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000000888-0000000000000001123
2017-03-25 00:42:44,272 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1124
2017-03-25 00:42:48,476 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 94.34 KB/s
2017-03-25 00:42:48,476 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001123 size 5423 bytes.
2017-03-25 00:42:48,537 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 887
2017-03-25 00:42:48,537 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000822, cpktTxId=0000000000000000822)
2017-03-25 00:49:56,339 INFO BlockStateChange: BLOCK* processReport: from storage DS-d832d0b9-b0fc-4e15-b505-a192e216a448 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0), blocks: 64, hasStaleStorage: false, processing time: 14 msecs
2017-03-25 00:50:20,350 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 83 
2017-03-25 00:50:20,365 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/32.csv
2017-03-25 00:50:20,382 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:50:20,395 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/32.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:51:06,034 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/33.csv
2017-03-25 00:51:06,083 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 00:51:06,086 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/33.csv is closed by DFSClient_NONMAPREDUCE_-89732091_1
2017-03-25 00:52:00,348 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-25 00:52:00,564 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-25 11:04:12,040 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-25 11:04:12,061 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-25 11:04:12,066 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-03-25 11:04:12,566 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-25 11:04:12,727 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-25 11:04:12,728 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-03-25 11:04:12,733 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://127.0.0.1:9000
2017-03-25 11:04:12,733 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 127.0.0.1:9000 to access this namenode/service.
2017-03-25 11:04:13,709 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-03-25 11:04:13,887 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-25 11:04:13,914 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-25 11:04:13,937 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-03-25 11:04:13,944 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-25 11:04:13,948 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-03-25 11:04:13,948 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-25 11:04:13,948 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-25 11:04:14,187 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-03-25 11:04:14,194 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-03-25 11:04:14,308 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-03-25 11:04:14,308 INFO org.mortbay.log: jetty-6.1.26
2017-03-25 11:04:14,671 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-03-25 11:04:15,156 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-25 11:04:15,157 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-03-25 11:04:15,231 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-25 11:04:15,231 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-25 11:04:15,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-25 11:04:15,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-25 11:04:15,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-25 11:04:15,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 25 11:04:15
2017-03-25 11:04:15,336 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-25 11:04:15,336 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-25 11:04:15,350 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-25 11:04:15,350 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-25 11:04:15,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-25 11:04:15,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-25 11:04:15,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-25 11:04:15,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-25 11:04:15,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-25 11:04:15,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-25 11:04:15,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-25 11:04:15,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-25 11:04:15,368 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2017-03-25 11:04:15,368 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-25 11:04:15,368 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-25 11:04:15,368 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-25 11:04:15,369 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-25 11:04:15,581 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-25 11:04:15,581 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-25 11:04:15,581 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-25 11:04:15,581 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-25 11:04:15,582 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-25 11:04:15,583 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-25 11:04:15,583 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-25 11:04:15,583 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-25 11:04:15,600 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-25 11:04:15,600 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-25 11:04:15,600 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-25 11:04:15,600 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-25 11:04:15,602 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-25 11:04:15,602 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-25 11:04:15,602 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-25 11:04:15,619 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-25 11:04:15,619 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-25 11:04:15,619 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-25 11:04:15,621 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-03-25 11:04:15,621 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-03-25 11:04:15,636 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-03-25 11:04:15,636 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-25 11:04:15,636 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-03-25 11:04:15,637 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-03-25 11:04:15,727 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/in_use.lock acquired by nodename 3042@gaurav-Inspiron-3542
2017-03-25 11:04:15,882 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current
2017-03-25 11:04:16,106 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_inprogress_0000000000000001124 -> /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000001124-0000000000000001144
2017-03-25 11:04:16,122 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000001123, cpktTxId=0000000000000001123)
2017-03-25 11:04:16,257 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 76 INodes.
2017-03-25 11:04:16,317 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-25 11:04:16,317 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 1123 from /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000001123
2017-03-25 11:04:16,317 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@463620de expecting start txid #1124
2017-03-25 11:04:16,317 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000001124-0000000000000001144
2017-03-25 11:04:16,319 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000001124-0000000000000001144' to transaction ID 1124
2017-03-25 11:04:16,397 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/edits_0000000000000001124-0000000000000001144 of size 1048576 edits # 21 loaded in 0 seconds
2017-03-25 11:04:16,398 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2017-03-25 11:04:16,398 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2017-03-25 11:04:16,403 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage.ckpt_0000000000000001144 using no compression
2017-03-25 11:04:16,441 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage.ckpt_0000000000000001144 of size 5390 bytes saved in 0 seconds.
2017-03-25 11:04:16,486 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1123
2017-03-25 11:04:16,486 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop/Desktop/hadoop-2.7.3/hdfs/namenode/current/fsimage_0000000000000000887, cpktTxId=0000000000000000887)
2017-03-25 11:04:16,565 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1145
2017-03-25 11:04:16,696 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-25 11:04:16,696 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1041 msecs
2017-03-25 11:04:17,016 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2017-03-25 11:04:17,026 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-03-25 11:04:17,036 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-03-25 11:04:17,195 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-03-25 11:04:17,202 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-25 11:04:17,202 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-03-25 11:04:17,202 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 66 blocks to reach the threshold 0.9990 of total blocks 66.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-03-25 11:04:17,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-25 11:04:17,251 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-03-25 11:04:17,251 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-03-25 11:04:17,272 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2017-03-25 11:04:17,272 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-03-25 11:04:17,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-03-25 11:04:22,316 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0) storage 94c459fd-27e9-4f2a-9627-5d1c3b12d5ea
2017-03-25 11:04:22,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-25 11:04:22,317 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2017-03-25 11:04:22,417 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-03-25 11:04:22,417 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d832d0b9-b0fc-4e15-b505-a192e216a448 for DN 127.0.0.1:50010
2017-03-25 11:04:22,463 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 65 has reached the threshold 0.9990 of total blocks 66. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2017-03-25 11:04:22,463 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-03-25 11:04:22,464 INFO BlockStateChange: BLOCK* processReport: from storage DS-d832d0b9-b0fc-4e15-b505-a192e216a448 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=94c459fd-27e9-4f2a-9627-5d1c3b12d5ea, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-6c520c85-0e84-4162-afa8-b7a3b7af144a;nsid=704566053;c=0), blocks: 66, hasStaleStorage: false, processing time: 8 msecs
2017-03-25 11:04:22,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 66
2017-03-25 11:04:22,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-03-25 11:04:22,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-03-25 11:04:22,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-03-25 11:04:22,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-03-25 11:04:22,468 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 4 msec
2017-03-25 11:04:42,469 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 66 has reached the threshold 0.9990 of total blocks 66. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2017-03-25 11:04:52,470 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 37 secs
2017-03-25 11:04:52,471 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2017-03-25 11:04:52,471 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2017-03-25 11:04:52,471 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-03-25 11:05:56,062 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 69 
2017-03-25 11:06:28,797 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/34.csv
2017-03-25 11:06:29,050 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741923_1099{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/34.csv
2017-03-25 11:06:29,064 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741923_1099{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 444583
2017-03-25 11:06:29,487 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/34.csv is closed by DFSClient_NONMAPREDUCE_2068174268_1
2017-03-25 11:06:38,869 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/35.csv
2017-03-25 11:06:38,884 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 11:06:38,887 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/35.csv is closed by DFSClient_NONMAPREDUCE_2068174268_1
2017-03-25 11:06:46,792 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/36.csv
2017-03-25 11:06:46,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 11:06:46,821 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/36.csv is closed by DFSClient_NONMAPREDUCE_2068174268_1
2017-03-25 11:06:54,892 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/40.csv
2017-03-25 11:06:54,942 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 11:06:54,955 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/40.csv is closed by DFSClient_NONMAPREDUCE_2068174268_1
2017-03-25 11:07:02,759 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 28 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 1 Number of syncs: 20 SyncTimes(ms): 310 
2017-03-25 11:07:02,782 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/37.csv
2017-03-25 11:07:02,797 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 11:07:02,811 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/37.csv is closed by DFSClient_NONMAPREDUCE_2068174268_1
2017-03-25 11:07:09,203 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} for /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/38.csv
2017-03-25 11:07:09,218 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d832d0b9-b0fc-4e15-b505-a192e216a448:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-03-25 11:07:09,222 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/hive/warehouse/patient/id=0a0c32c9e08cc2ea76a71649de56be6d/38.csv is closed by DFSClient_NONMAPREDUCE_2068174268_1
2017-03-25 11:09:15,791 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 40 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 1 Number of syncs: 28 SyncTimes(ms): 394 
2017-03-25 11:12:53,792 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1962ms
No GCs detected
2017-03-25 11:35:00,789 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1893ms
No GCs detected
2017-03-25 11:51:36,164 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-25 11:51:36,169 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
