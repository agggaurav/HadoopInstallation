2017-01-16 14:09:14,563 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop/:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-16 14:09:14,575 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-16 14:09:15,280 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-16 14:09:15,363 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-16 14:09:15,363 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-16 14:09:15,603 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop1/dfs/namesecondary/in_use.lock acquired by nodename 8392@gaurav-Inspiron-3542
2017-01-16 14:09:15,607 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-16 14:09:15,607 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-16 14:09:15,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-16 14:09:15,645 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-16 14:09:15,646 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-16 14:09:15,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 16 14:09:15
2017-01-16 14:09:15,650 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-16 14:09:15,650 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-16 14:09:15,652 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-16 14:09:15,652 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-16 14:09:15,672 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-16 14:09:15,673 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-16 14:09:15,673 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-16 14:09:15,673 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-16 14:09:15,673 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-16 14:09:15,673 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-16 14:09:15,673 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-16 14:09:15,673 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-16 14:09:15,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-16 14:09:15,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-16 14:09:15,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-16 14:09:15,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-16 14:09:15,677 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-16 14:09:15,719 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-16 14:09:15,719 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-16 14:09:15,719 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-16 14:09:15,719 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-16 14:09:15,720 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-16 14:09:15,720 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-16 14:09:15,720 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-16 14:09:15,721 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-16 14:09:15,727 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-16 14:09:15,727 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-16 14:09:15,727 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-16 14:09:15,727 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-16 14:09:15,729 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-16 14:09:15,729 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-16 14:09:15,729 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-16 14:09:15,731 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-16 14:09:15,732 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-16 14:09:15,732 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-16 14:09:15,743 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-16 14:09:15,813 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-16 14:09:15,828 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-16 14:09:15,837 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-16 14:09:15,847 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-16 14:09:15,851 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-16 14:09:15,851 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-16 14:09:15,851 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-16 14:09:15,873 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-16 14:09:15,873 INFO org.mortbay.log: jetty-6.1.26
2017-01-16 14:09:16,068 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-16 14:09:16,068 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-16 14:09:16,070 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-16 14:09:16,070 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-16 14:10:17,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:10:18,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:10:19,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:10:20,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:10:21,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:10:22,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:10:23,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:10:24,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:10:25,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:10:26,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:10:26,132 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-01-16 14:11:27,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:11:28,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:11:29,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:11:30,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:11:31,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:11:32,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:11:33,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:11:34,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:11:35,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:11:36,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:11:36,146 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-01-16 14:12:37,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:12:38,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:12:39,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:12:40,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:12:41,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:12:42,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:12:43,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:12:44,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:12:45,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:12:46,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-01-16 14:12:46,159 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-01-16 14:13:38,405 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-16 14:13:38,406 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-01-16 14:15:38,902 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop/:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-16 14:15:38,914 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-16 14:15:39,607 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-16 14:15:39,687 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-16 14:15:39,687 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-16 14:15:39,950 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop1/dfs/namesecondary/in_use.lock acquired by nodename 10076@gaurav-Inspiron-3542
2017-01-16 14:15:39,955 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-16 14:15:39,955 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-16 14:15:39,990 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-16 14:15:39,990 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-16 14:15:39,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-16 14:15:39,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 16 14:15:39
2017-01-16 14:15:39,995 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-16 14:15:39,995 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-16 14:15:39,997 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-16 14:15:39,998 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-16 14:15:40,017 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-16 14:15:40,018 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-16 14:15:40,018 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-16 14:15:40,018 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-16 14:15:40,018 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-16 14:15:40,018 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-16 14:15:40,018 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-16 14:15:40,018 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-16 14:15:40,020 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-16 14:15:40,020 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-16 14:15:40,020 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-16 14:15:40,020 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-16 14:15:40,022 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-16 14:15:40,062 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-16 14:15:40,062 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-16 14:15:40,062 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-16 14:15:40,062 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-16 14:15:40,063 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-16 14:15:40,063 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-16 14:15:40,063 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-16 14:15:40,064 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-16 14:15:40,070 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-16 14:15:40,070 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-16 14:15:40,071 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-16 14:15:40,071 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-16 14:15:40,072 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-16 14:15:40,072 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-16 14:15:40,072 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-16 14:15:40,075 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-16 14:15:40,075 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-16 14:15:40,075 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-16 14:15:40,086 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-16 14:15:40,155 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-16 14:15:40,169 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-16 14:15:40,178 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-16 14:15:40,186 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-16 14:15:40,190 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-16 14:15:40,191 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-16 14:15:40,191 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-16 14:15:40,214 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-16 14:15:40,214 INFO org.mortbay.log: jetty-6.1.26
2017-01-16 14:15:40,375 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-16 14:15:40,375 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-16 14:15:40,377 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-16 14:15:40,377 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-16 14:16:40,715 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2017-01-16 14:16:41,302 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=0&storageInfo=-63:1430207621:0:CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d
2017-01-16 14:16:41,357 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2017-01-16 14:16:41,911 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.08s at 0.00 KB/s
2017-01-16 14:16:41,911 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 354 bytes.
2017-01-16 14:16:41,956 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1&endTxId=2&storageInfo=-63:1430207621:0:CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d
2017-01-16 14:16:41,989 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 0.00 KB/s
2017-01-16 14:16:41,989 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000001-0000000000000000002_0000000000116750031 size 0 bytes.
2017-01-16 14:16:42,029 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-01-16 14:16:42,061 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-01-16 14:16:42,062 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-hadoop1/dfs/namesecondary/current/fsimage_0000000000000000000
2017-01-16 14:16:42,062 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-01-16 14:16:42,067 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-01-16 14:16:42,070 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-hadoop1/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 expecting start txid #1
2017-01-16 14:16:42,071 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-hadoop1/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002
2017-01-16 14:16:42,084 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-hadoop1/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds
2017-01-16 14:16:42,089 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-hadoop1/dfs/namesecondary/current/fsimage.ckpt_0000000000000000002 using no compression
2017-01-16 14:16:42,120 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-hadoop1/dfs/namesecondary/current/fsimage.ckpt_0000000000000000002 of size 354 bytes saved in 0 seconds.
2017-01-16 14:16:42,157 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-hadoop1/dfs/namesecondary
2017-01-16 14:16:42,290 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2 to namenode at http://localhost:50070 in 0.1 seconds
2017-01-16 14:16:42,290 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 354
2017-01-16 14:41:08,588 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-16 14:41:08,590 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-01-18 11:30:11,465 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-18 11:30:11,510 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-18 11:30:12,226 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-18 11:30:12,304 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-18 11:30:12,304 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-18 11:30:12,588 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop1/dfs/namesecondary/in_use.lock acquired by nodename 31881@gaurav-Inspiron-3542
2017-01-18 11:30:12,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-18 11:30:12,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-18 11:30:12,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-18 11:30:12,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-18 11:30:12,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-18 11:30:12,738 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 18 11:30:12
2017-01-18 11:30:12,740 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-18 11:30:12,740 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-18 11:30:12,741 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-18 11:30:12,741 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-18 11:30:12,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-18 11:30:12,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-18 11:30:12,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-18 11:30:12,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-18 11:30:12,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-18 11:30:12,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-18 11:30:12,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-18 11:30:12,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-18 11:30:12,763 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-18 11:30:12,763 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-18 11:30:12,763 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-18 11:30:12,763 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-18 11:30:12,765 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-18 11:30:12,802 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-18 11:30:12,802 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-18 11:30:12,803 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-18 11:30:12,803 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-18 11:30:12,804 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-18 11:30:12,804 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-18 11:30:12,804 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-18 11:30:12,804 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-18 11:30:12,810 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-18 11:30:12,810 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-18 11:30:12,810 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-18 11:30:12,810 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-18 11:30:12,811 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-18 11:30:12,811 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-18 11:30:12,811 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-18 11:30:12,814 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-18 11:30:12,814 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-18 11:30:12,814 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-18 11:30:12,825 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-18 11:30:12,890 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-18 11:30:12,899 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-18 11:30:12,905 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-18 11:30:12,911 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-18 11:30:12,914 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-18 11:30:12,914 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-18 11:30:12,914 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-18 11:30:12,932 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-18 11:30:12,932 INFO org.mortbay.log: jetty-6.1.26
2017-01-18 11:30:13,140 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-18 11:30:13,140 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-18 11:30:13,143 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-18 11:30:13,143 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-18 11:31:13,402 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:32:13,539 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:33:13,695 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:34:13,849 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:35:14,023 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:36:14,156 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:37:14,379 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:38:14,568 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:39:14,757 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:40:14,914 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:41:15,060 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:42:15,193 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:43:15,324 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:44:15,487 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:45:15,619 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:46:15,784 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:47:15,927 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:48:16,071 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:49:16,203 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:50:16,367 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:51:16,508 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:52:16,707 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:53:16,862 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:54:17,010 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:55:17,255 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:56:17,413 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:57:17,543 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:58:17,692 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 11:59:17,881 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:00:18,101 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:01:18,244 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:02:18,399 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:03:18,553 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:04:18,695 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:05:18,860 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:06:19,013 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:07:19,145 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:08:19,280 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:09:19,421 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:10:19,587 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:11:19,750 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:12:19,869 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:13:20,057 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:14:20,186 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:14:49,482 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-18 12:14:49,484 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-01-18 12:19:15,502 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-18 12:19:15,513 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-18 12:19:16,218 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-18 12:19:16,293 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-18 12:19:16,293 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-18 12:19:16,566 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop1/dfs/namesecondary/in_use.lock acquired by nodename 2082@gaurav-Inspiron-3542
2017-01-18 12:19:16,676 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-18 12:19:16,677 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-18 12:19:16,747 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-18 12:19:16,747 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-18 12:19:16,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-18 12:19:16,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 18 12:19:16
2017-01-18 12:19:16,753 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-18 12:19:16,754 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-18 12:19:16,756 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-18 12:19:16,756 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-18 12:19:16,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-18 12:19:16,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-18 12:19:16,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-18 12:19:16,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-18 12:19:16,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-18 12:19:16,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-18 12:19:16,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-18 12:19:16,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-18 12:19:16,788 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-18 12:19:16,788 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-18 12:19:16,788 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-18 12:19:16,788 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-18 12:19:16,792 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-18 12:19:16,858 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-18 12:19:16,859 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-18 12:19:16,859 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-18 12:19:16,859 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-18 12:19:16,860 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-18 12:19:16,860 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-18 12:19:16,860 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-18 12:19:16,860 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-18 12:19:16,868 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-18 12:19:16,868 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-18 12:19:16,868 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-18 12:19:16,868 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-18 12:19:16,870 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-18 12:19:16,870 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-18 12:19:16,870 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-18 12:19:16,873 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-18 12:19:16,873 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-18 12:19:16,873 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-18 12:19:16,886 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-18 12:19:16,968 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-18 12:19:16,982 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-18 12:19:16,990 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-18 12:19:16,998 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-18 12:19:17,001 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-18 12:19:17,001 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-18 12:19:17,001 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-18 12:19:17,015 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-18 12:19:17,015 INFO org.mortbay.log: jetty-6.1.26
2017-01-18 12:19:17,210 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-18 12:19:17,210 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-18 12:19:17,213 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-18 12:19:17,213 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-18 12:20:17,467 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:21:17,608 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:22:17,775 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:23:17,926 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:24:18,112 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:25:18,255 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:26:18,408 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:27:18,554 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:28:18,739 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:29:18,880 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:30:19,023 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:31:19,174 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:32:19,308 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:33:19,484 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:34:19,627 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:35:19,779 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:36:19,919 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:37:20,063 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:38:20,184 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:39:20,340 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:40:20,504 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:41:20,647 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:42:20,801 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:43:20,941 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:44:21,096 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:45:21,270 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:46:21,402 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:47:21,554 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:48:21,710 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:49:21,862 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:50:22,003 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:51:22,145 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:52:22,287 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:53:22,428 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:54:22,571 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:55:22,712 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:56:22,856 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:57:23,011 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:58:23,146 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 12:59:23,277 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:00:23,432 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:01:23,561 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:02:23,707 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:03:23,848 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:04:23,992 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:05:24,143 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:06:24,283 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:07:24,428 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:08:24,571 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:09:24,726 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:10:24,856 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:11:24,989 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:12:25,131 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:13:25,284 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:14:25,438 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:15:25,590 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:16:25,817 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:17:26,044 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:18:26,183 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:19:26,315 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:20:26,458 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:21:26,615 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:22:26,742 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:23:26,869 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:24:27,019 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 13:25:27,183 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:07:49,430 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:08:49,587 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:09:49,704 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:10:49,852 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:11:49,981 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:12:50,103 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:13:50,217 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:14:50,360 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:15:50,503 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:16:50,633 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:17:50,819 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:18:50,980 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:19:51,137 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:20:51,287 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:21:51,424 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:22:51,558 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:23:51,692 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:24:51,844 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:25:52,033 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:26:52,158 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:27:52,279 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:28:52,424 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:29:52,574 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:30:52,714 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:31:52,856 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:32:52,980 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:33:53,116 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:34:53,304 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:35:53,431 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:36:53,547 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:37:53,732 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:38:53,894 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:39:54,029 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:40:54,163 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:41:54,285 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:42:54,412 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:43:54,527 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:44:54,723 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:45:54,849 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:46:54,964 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:47:55,081 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:48:55,221 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:49:55,380 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:50:55,502 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:51:55,671 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:52:55,803 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:53:55,951 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:54:56,084 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:55:56,377 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:56:56,548 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:57:56,682 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:58:56,839 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 21:59:56,981 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:00:57,106 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:01:57,390 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:02:57,525 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:03:57,667 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:04:57,820 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:05:57,940 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:06:58,115 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:07:58,246 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:08:58,388 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:09:58,522 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:10:58,646 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:11:58,884 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:12:59,041 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:13:59,166 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:14:59,324 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:15:59,484 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:16:59,655 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:17:59,930 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:19:00,454 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:20:00,717 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:21:00,926 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:22:01,159 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:23:01,337 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:24:01,582 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:25:01,807 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:26:01,985 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:27:02,127 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:28:02,308 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:29:02,448 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:30:02,681 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:31:02,851 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:32:03,048 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:33:03,214 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:34:03,374 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:35:03,561 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:36:03,717 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:37:03,847 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:38:04,001 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:39:04,157 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:40:04,280 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:41:04,444 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:42:04,611 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:43:04,764 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:44:04,887 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:45:05,029 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:46:05,216 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:47:05,686 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:48:05,827 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:49:05,952 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:50:06,086 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:51:06,228 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:52:06,361 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:53:06,493 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:54:06,649 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:55:06,768 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:56:06,889 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:57:07,009 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:58:07,150 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 22:59:07,360 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:00:07,506 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:01:07,625 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:02:07,779 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:03:07,900 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:04:08,032 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:05:08,175 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:06:08,339 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:07:08,872 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:08:08,993 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:09:09,115 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:10:09,237 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:11:09,359 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:12:09,535 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:13:09,656 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:14:09,816 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:15:09,988 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:16:10,115 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:17:10,240 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:18:10,363 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:19:10,500 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:20:10,638 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:21:10,782 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:22:11,024 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:23:11,241 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:24:11,433 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:25:11,579 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:26:11,704 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:27:11,991 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:28:12,152 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:29:12,308 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:30:12,431 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:31:12,553 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:32:12,694 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:33:12,824 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:34:12,988 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:35:13,130 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:36:13,272 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:37:13,393 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:38:13,535 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:39:13,668 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:40:13,812 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:41:13,936 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:42:14,140 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:43:14,295 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:44:14,467 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:45:14,612 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:46:14,733 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:47:14,885 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:48:15,044 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:49:15,177 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:50:15,343 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:51:15,473 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:52:15,605 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:53:15,746 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:54:15,961 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:55:16,102 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:56:16,246 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:57:16,380 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:58:16,536 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 23:59:16,688 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:00:16,832 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:01:16,972 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:02:17,116 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:03:17,256 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:04:17,393 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:05:17,522 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:06:17,650 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:07:17,862 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:08:18,012 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:09:18,384 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:10:18,513 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:11:18,662 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:12:18,798 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:13:18,926 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:14:19,063 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:15:19,190 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:16:19,322 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:17:19,490 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:18:19,650 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 00:19:19,785 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:08:48,595 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:09:48,814 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:10:48,949 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:11:49,095 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:12:49,228 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:13:49,363 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:14:49,508 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:15:49,650 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:16:49,787 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:17:49,912 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:18:50,107 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:19:50,252 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:20:50,406 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:21:50,552 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:22:50,717 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:23:50,892 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:24:51,331 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:25:51,528 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:26:51,703 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:27:51,840 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:28:51,992 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:29:52,128 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:30:52,395 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:31:52,585 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:32:52,743 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:33:52,944 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:34:53,079 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:35:53,214 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:36:53,337 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:37:53,483 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:38:53,631 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:39:53,775 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:40:53,921 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:41:54,056 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:42:54,232 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:43:54,366 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 01:44:54,502 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:18:21,626 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:19:21,762 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:20:21,916 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:21:22,049 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:21:32,454 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-19 13:21:32,491 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-01-19 13:22:31,921 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-19 13:22:31,932 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-19 13:22:32,644 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-19 13:22:32,727 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-19 13:22:32,727 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-19 13:22:32,992 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop1/dfs/namesecondary/in_use.lock acquired by nodename 11703@gaurav-Inspiron-3542
2017-01-19 13:22:33,094 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-19 13:22:33,094 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-19 13:22:33,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-19 13:22:33,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-19 13:22:33,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-19 13:22:33,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 19 13:22:33
2017-01-19 13:22:33,167 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-19 13:22:33,167 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:22:33,169 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-19 13:22:33,169 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-19 13:22:33,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-19 13:22:33,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-19 13:22:33,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-19 13:22:33,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-19 13:22:33,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-19 13:22:33,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-19 13:22:33,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-19 13:22:33,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-19 13:22:33,203 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-19 13:22:33,203 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-19 13:22:33,203 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-19 13:22:33,203 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-19 13:22:33,207 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-19 13:22:33,275 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-19 13:22:33,276 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:22:33,276 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-19 13:22:33,276 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-19 13:22:33,277 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-19 13:22:33,277 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-19 13:22:33,277 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-19 13:22:33,277 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-19 13:22:33,284 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-19 13:22:33,285 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:22:33,285 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-19 13:22:33,285 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-19 13:22:33,286 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-19 13:22:33,286 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-19 13:22:33,286 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-19 13:22:33,289 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-19 13:22:33,289 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-19 13:22:33,289 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-19 13:22:33,303 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-19 13:22:33,385 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-19 13:22:33,400 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-19 13:22:33,409 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-19 13:22:33,415 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-19 13:22:33,417 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-19 13:22:33,417 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-19 13:22:33,417 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-19 13:22:33,432 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-19 13:22:33,432 INFO org.mortbay.log: jetty-6.1.26
2017-01-19 13:22:33,639 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-19 13:22:33,640 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-19 13:22:33,640 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-19 13:22:33,640 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-19 13:23:33,856 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:24:34,007 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:25:34,151 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:26:34,296 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:27:34,458 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:28:34,624 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:29:34,766 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:30:34,931 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:31:35,066 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:32:35,218 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:33:31,720 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-19 13:33:31,722 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-01-19 13:34:33,355 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-19 13:34:33,367 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-19 13:34:34,086 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-19 13:34:34,161 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-19 13:34:34,161 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-19 13:34:34,421 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop1/dfs/namesecondary/in_use.lock acquired by nodename 13648@gaurav-Inspiron-3542
2017-01-19 13:34:34,497 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-19 13:34:34,497 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-19 13:34:34,545 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-19 13:34:34,545 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-19 13:34:34,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-19 13:34:34,548 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 19 13:34:34
2017-01-19 13:34:34,550 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-19 13:34:34,550 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:34:34,552 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-19 13:34:34,552 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-19 13:34:34,586 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-19 13:34:34,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-19 13:34:34,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-19 13:34:34,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-19 13:34:34,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-19 13:34:34,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-19 13:34:34,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-19 13:34:34,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-19 13:34:34,590 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-19 13:34:34,590 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-19 13:34:34,590 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-19 13:34:34,590 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-19 13:34:34,594 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-19 13:34:34,662 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-19 13:34:34,662 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:34:34,662 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-19 13:34:34,662 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-19 13:34:34,663 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-19 13:34:34,663 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-19 13:34:34,663 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-19 13:34:34,663 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-19 13:34:34,672 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-19 13:34:34,672 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:34:34,673 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-19 13:34:34,673 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-19 13:34:34,674 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-19 13:34:34,674 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-19 13:34:34,674 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-19 13:34:34,678 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-19 13:34:34,678 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-19 13:34:34,678 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-19 13:34:34,694 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-19 13:34:34,793 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-19 13:34:34,807 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-19 13:34:34,815 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-19 13:34:34,822 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-19 13:34:34,825 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-19 13:34:34,825 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-19 13:34:34,825 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-19 13:34:34,842 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-19 13:34:34,843 INFO org.mortbay.log: jetty-6.1.26
2017-01-19 13:34:34,996 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-19 13:34:34,996 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-19 13:34:34,997 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-19 13:34:34,997 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-19 13:35:01,922 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-19 13:35:01,923 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-01-19 13:41:24,561 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-19 13:41:24,573 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-19 13:41:25,295 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-19 13:41:25,366 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-19 13:41:25,366 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-19 13:41:25,628 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop1/dfs/namesecondary/in_use.lock acquired by nodename 16306@gaurav-Inspiron-3542
2017-01-19 13:41:25,721 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-19 13:41:25,721 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-19 13:41:25,793 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-19 13:41:25,793 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-19 13:41:25,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-19 13:41:25,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 19 13:41:25
2017-01-19 13:41:25,801 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-19 13:41:25,801 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:41:25,804 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-19 13:41:25,804 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-19 13:41:25,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-19 13:41:25,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-19 13:41:25,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-19 13:41:25,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-19 13:41:25,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-19 13:41:25,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-19 13:41:25,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-19 13:41:25,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-19 13:41:25,837 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-19 13:41:25,837 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-19 13:41:25,837 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-19 13:41:25,838 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-19 13:41:25,840 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-19 13:41:25,890 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-19 13:41:25,890 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:41:25,891 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-19 13:41:25,891 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-19 13:41:25,891 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-19 13:41:25,891 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-19 13:41:25,892 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-19 13:41:25,892 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-19 13:41:25,898 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-19 13:41:25,898 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:41:25,899 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-19 13:41:25,899 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-19 13:41:25,900 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-19 13:41:25,900 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-19 13:41:25,900 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-19 13:41:25,902 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-19 13:41:25,902 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-19 13:41:25,902 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-19 13:41:25,914 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-19 13:41:25,994 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-19 13:41:26,005 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-19 13:41:26,011 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-19 13:41:26,017 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-19 13:41:26,020 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-19 13:41:26,020 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-19 13:41:26,020 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-19 13:41:26,036 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-19 13:41:26,036 INFO org.mortbay.log: jetty-6.1.26
2017-01-19 13:41:26,183 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-19 13:41:26,183 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-19 13:41:26,183 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-19 13:41:26,184 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-19 13:42:26,428 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1148388252 cTime = 0 ; clusterId = CID-dcb91224-1e3b-4e1e-8152-912cbd5a6cec ; blockpoolId = BP-490033676-127.0.1.1-1484685011924.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 13:43:27,007 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-19 13:43:27,009 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-01-19 13:53:22,477 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-19 13:53:22,486 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-19 13:53:23,197 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-19 13:53:23,277 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-19 13:53:23,277 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-19 13:53:23,531 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop1/dfs/namesecondary/in_use.lock acquired by nodename 18760@gaurav-Inspiron-3542
2017-01-19 13:53:23,605 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-19 13:53:23,605 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-19 13:53:23,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-19 13:53:23,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-19 13:53:23,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-19 13:53:23,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 19 13:53:23
2017-01-19 13:53:23,657 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-19 13:53:23,657 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:53:23,659 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-19 13:53:23,659 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-19 13:53:23,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-19 13:53:23,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-19 13:53:23,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-19 13:53:23,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-19 13:53:23,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-19 13:53:23,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-19 13:53:23,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-19 13:53:23,691 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-19 13:53:23,695 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-19 13:53:23,695 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-19 13:53:23,695 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-19 13:53:23,695 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-19 13:53:23,699 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-19 13:53:23,764 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-19 13:53:23,764 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:53:23,764 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-19 13:53:23,764 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-19 13:53:23,765 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-19 13:53:23,765 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-19 13:53:23,766 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-19 13:53:23,766 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-19 13:53:23,775 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-19 13:53:23,775 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 13:53:23,775 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-19 13:53:23,775 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-19 13:53:23,777 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-19 13:53:23,777 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-19 13:53:23,777 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-19 13:53:23,781 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-19 13:53:23,781 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-19 13:53:23,781 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-19 13:53:23,797 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-19 13:53:23,896 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-19 13:53:23,910 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-19 13:53:23,919 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-19 13:53:23,928 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-19 13:53:23,932 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-19 13:53:23,932 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-19 13:53:23,933 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-19 13:53:23,956 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-19 13:53:23,956 INFO org.mortbay.log: jetty-6.1.26
2017-01-19 13:53:24,119 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-19 13:53:24,119 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-19 13:53:24,119 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-19 13:53:24,119 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-19 13:54:01,980 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-19 13:54:01,982 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-01-19 14:02:39,215 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-19 14:02:39,227 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-19 14:02:39,953 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-19 14:02:40,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-19 14:02:40,043 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-19 14:02:40,316 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop1/dfs/namesecondary/in_use.lock acquired by nodename 20314@gaurav-Inspiron-3542
2017-01-19 14:02:40,409 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-19 14:02:40,409 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-19 14:02:40,475 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-19 14:02:40,475 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-19 14:02:40,477 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-19 14:02:40,480 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 19 14:02:40
2017-01-19 14:02:40,482 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-19 14:02:40,483 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:02:40,485 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-19 14:02:40,485 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-19 14:02:40,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-19 14:02:40,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-19 14:02:40,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-19 14:02:40,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-19 14:02:40,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-19 14:02:40,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-19 14:02:40,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-19 14:02:40,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-19 14:02:40,511 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-19 14:02:40,511 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-19 14:02:40,511 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-19 14:02:40,511 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-19 14:02:40,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-19 14:02:40,578 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-19 14:02:40,578 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:02:40,578 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-19 14:02:40,578 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-19 14:02:40,579 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-19 14:02:40,580 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-19 14:02:40,580 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-19 14:02:40,580 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-19 14:02:40,590 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-19 14:02:40,590 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:02:40,590 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-19 14:02:40,590 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-19 14:02:40,591 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-19 14:02:40,591 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-19 14:02:40,591 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-19 14:02:40,595 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-19 14:02:40,595 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-19 14:02:40,595 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-19 14:02:40,609 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-19 14:02:40,696 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-19 14:02:40,710 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-19 14:02:40,717 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-19 14:02:40,723 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-19 14:02:40,726 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-19 14:02:40,726 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-19 14:02:40,726 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-19 14:02:40,740 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-19 14:02:40,740 INFO org.mortbay.log: jetty-6.1.26
2017-01-19 14:02:40,898 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-19 14:02:40,898 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-19 14:02:40,899 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-19 14:02:40,899 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-19 14:03:41,191 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 657117488 cTime = 0 ; clusterId = CID-2fab8d3b-411e-42ef-8e8c-46a38d0b09bf ; blockpoolId = BP-1652495193-127.0.1.1-1484814716237.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:04:41,355 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 657117488 cTime = 0 ; clusterId = CID-2fab8d3b-411e-42ef-8e8c-46a38d0b09bf ; blockpoolId = BP-1652495193-127.0.1.1-1484814716237.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:05:41,489 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 657117488 cTime = 0 ; clusterId = CID-2fab8d3b-411e-42ef-8e8c-46a38d0b09bf ; blockpoolId = BP-1652495193-127.0.1.1-1484814716237.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:06:21,696 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-19 14:06:21,698 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-01-19 14:11:17,514 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-19 14:11:17,526 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-19 14:11:18,185 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-19 14:11:18,258 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-19 14:11:18,258 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-19 14:11:18,514 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-hadoop1/dfs/namesecondary/in_use.lock acquired by nodename 22711@gaurav-Inspiron-3542
2017-01-19 14:11:18,610 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-19 14:11:18,610 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-19 14:11:18,671 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-19 14:11:18,671 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-19 14:11:18,673 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-19 14:11:18,675 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 19 14:11:18
2017-01-19 14:11:18,678 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-19 14:11:18,678 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:11:18,680 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-19 14:11:18,681 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-19 14:11:18,707 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-19 14:11:18,707 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-19 14:11:18,707 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-19 14:11:18,707 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-19 14:11:18,707 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-19 14:11:18,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-19 14:11:18,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-19 14:11:18,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-19 14:11:18,710 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-19 14:11:18,710 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-19 14:11:18,710 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-19 14:11:18,711 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-19 14:11:18,714 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-19 14:11:18,768 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-19 14:11:18,768 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:11:18,768 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-19 14:11:18,768 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-19 14:11:18,769 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-19 14:11:18,769 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-19 14:11:18,769 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-19 14:11:18,770 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-19 14:11:18,779 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-19 14:11:18,779 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:11:18,779 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-19 14:11:18,779 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-19 14:11:18,781 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-19 14:11:18,781 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-19 14:11:18,781 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-19 14:11:18,785 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-19 14:11:18,785 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-19 14:11:18,785 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-19 14:11:18,800 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-19 14:11:18,896 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-19 14:11:18,910 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-19 14:11:18,918 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-19 14:11:18,925 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-19 14:11:18,929 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-19 14:11:18,929 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-19 14:11:18,929 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-19 14:11:18,949 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-19 14:11:18,949 INFO org.mortbay.log: jetty-6.1.26
2017-01-19 14:11:19,112 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-19 14:11:19,112 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-19 14:11:19,113 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-19 14:11:19,113 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-19 14:12:19,434 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1175582311 cTime = 0 ; clusterId = CID-42257bed-939a-4341-acbf-d534fea9a8ec ; blockpoolId = BP-2145950208-127.0.1.1-1484815170761.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:13:19,594 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1175582311 cTime = 0 ; clusterId = CID-42257bed-939a-4341-acbf-d534fea9a8ec ; blockpoolId = BP-2145950208-127.0.1.1-1484815170761.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:14:19,762 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1175582311 cTime = 0 ; clusterId = CID-42257bed-939a-4341-acbf-d534fea9a8ec ; blockpoolId = BP-2145950208-127.0.1.1-1484815170761.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:15:19,904 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1175582311 cTime = 0 ; clusterId = CID-42257bed-939a-4341-acbf-d534fea9a8ec ; blockpoolId = BP-2145950208-127.0.1.1-1484815170761.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:16:20,102 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1175582311 cTime = 0 ; clusterId = CID-42257bed-939a-4341-acbf-d534fea9a8ec ; blockpoolId = BP-2145950208-127.0.1.1-1484815170761.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:17:20,245 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1175582311 cTime = 0 ; clusterId = CID-42257bed-939a-4341-acbf-d534fea9a8ec ; blockpoolId = BP-2145950208-127.0.1.1-1484815170761.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:18:20,442 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1175582311 cTime = 0 ; clusterId = CID-42257bed-939a-4341-acbf-d534fea9a8ec ; blockpoolId = BP-2145950208-127.0.1.1-1484815170761.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:19:20,589 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1175582311 cTime = 0 ; clusterId = CID-42257bed-939a-4341-acbf-d534fea9a8ec ; blockpoolId = BP-2145950208-127.0.1.1-1484815170761.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:20:20,720 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1175582311 cTime = 0 ; clusterId = CID-42257bed-939a-4341-acbf-d534fea9a8ec ; blockpoolId = BP-2145950208-127.0.1.1-1484815170761.
Expecting respectively: -63; 1430207621; 0; CID-c1c5f4a1-45ea-44cf-b335-de976e452e8d; BP-2021495725-127.0.1.1-1484556314266.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-01-19 14:20:31,586 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-19 14:20:31,588 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-01-19 14:24:59,785 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-19 14:24:59,798 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-19 14:25:00,470 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-19 14:25:00,546 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-19 14:25:00,547 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-19 14:25:00,812 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/in_use.lock acquired by nodename 25739@gaurav-Inspiron-3542
2017-01-19 14:25:00,816 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-19 14:25:00,816 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-19 14:25:00,851 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-19 14:25:00,851 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-19 14:25:00,852 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-19 14:25:00,853 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 19 14:25:00
2017-01-19 14:25:00,855 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-19 14:25:00,855 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:25:00,857 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-19 14:25:00,857 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-19 14:25:00,876 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-19 14:25:00,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-19 14:25:00,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-19 14:25:00,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-19 14:25:00,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-19 14:25:00,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-19 14:25:00,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-19 14:25:00,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-19 14:25:00,878 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-19 14:25:00,878 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-19 14:25:00,879 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-19 14:25:00,879 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-19 14:25:00,881 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-19 14:25:00,919 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-19 14:25:00,919 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:25:00,919 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-19 14:25:00,919 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-19 14:25:00,920 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-19 14:25:00,920 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-19 14:25:00,920 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-19 14:25:00,920 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-19 14:25:00,926 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-19 14:25:00,927 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:25:00,927 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-19 14:25:00,927 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-19 14:25:00,928 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-19 14:25:00,928 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-19 14:25:00,928 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-19 14:25:00,931 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-19 14:25:00,931 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-19 14:25:00,931 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-19 14:25:00,942 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-19 14:25:01,000 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-19 14:25:01,010 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-19 14:25:01,016 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-19 14:25:01,022 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-19 14:25:01,026 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-19 14:25:01,026 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-19 14:25:01,026 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-19 14:25:01,044 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-19 14:25:01,044 INFO org.mortbay.log: jetty-6.1.26
2017-01-19 14:25:01,203 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-19 14:25:01,203 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-19 14:25:01,204 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-19 14:25:01,204 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-19 14:26:01,519 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2017-01-19 14:26:02,053 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=0&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-01-19 14:26:02,110 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2017-01-19 14:26:02,664 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.09s at 0.00 KB/s
2017-01-19 14:26:02,664 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 354 bytes.
2017-01-19 14:26:02,699 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1&endTxId=2&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-01-19 14:26:02,732 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 0.00 KB/s
2017-01-19 14:26:02,732 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000001-0000000000000000002_0000000000185737910 size 0 bytes.
2017-01-19 14:26:02,769 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-01-19 14:26:02,800 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-01-19 14:26:02,800 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000000
2017-01-19 14:26:02,800 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-01-19 14:26:02,806 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-01-19 14:26:02,810 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 expecting start txid #1
2017-01-19 14:26:02,810 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002
2017-01-19 14:26:02,821 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds
2017-01-19 14:26:02,827 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000002 using no compression
2017-01-19 14:26:02,855 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000002 of size 354 bytes saved in 0 seconds.
2017-01-19 14:26:02,889 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary
2017-01-19 14:26:03,041 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2 to namenode at http://localhost:50070 in 0.108 seconds
2017-01-19 14:26:03,041 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 354
2017-01-19 14:28:27,655 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-19 14:28:27,656 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-01-19 14:29:45,423 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-01-19 14:29:45,432 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-19 14:29:46,158 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-19 14:29:46,245 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-19 14:29:46,245 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-01-19 14:29:46,502 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/in_use.lock acquired by nodename 27900@gaurav-Inspiron-3542
2017-01-19 14:29:46,580 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-19 14:29:46,580 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-19 14:29:46,627 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-19 14:29:46,627 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-19 14:29:46,628 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-19 14:29:46,630 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Jan 19 14:29:46
2017-01-19 14:29:46,632 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-19 14:29:46,632 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:29:46,634 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-19 14:29:46,634 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-19 14:29:46,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-19 14:29:46,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-19 14:29:46,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-19 14:29:46,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-19 14:29:46,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-19 14:29:46,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-19 14:29:46,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-19 14:29:46,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-19 14:29:46,661 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-01-19 14:29:46,661 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-19 14:29:46,661 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-19 14:29:46,661 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-19 14:29:46,664 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-19 14:29:46,711 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-19 14:29:46,711 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:29:46,712 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-19 14:29:46,712 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-19 14:29:46,713 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-19 14:29:46,713 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-19 14:29:46,713 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-19 14:29:46,713 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-19 14:29:46,721 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-19 14:29:46,721 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-19 14:29:46,721 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-19 14:29:46,721 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-19 14:29:46,723 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-19 14:29:46,723 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-19 14:29:46,723 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-19 14:29:46,727 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-19 14:29:46,727 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-19 14:29:46,727 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-19 14:29:46,741 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-01-19 14:29:46,821 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-19 14:29:46,833 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-19 14:29:46,839 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-01-19 14:29:46,845 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-19 14:29:46,849 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-01-19 14:29:46,849 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-19 14:29:46,849 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-19 14:29:46,867 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-01-19 14:29:46,867 INFO org.mortbay.log: jetty-6.1.26
2017-01-19 14:29:47,018 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-01-19 14:29:47,018 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-01-19 14:29:47,019 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-01-19 14:29:47,019 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-01-19 14:30:47,335 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2017-01-19 14:30:47,691 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=2&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-01-19 14:30:47,719 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2017-01-19 14:30:48,116 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2017-01-19 14:30:48,117 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 354 bytes.
2017-01-19 14:30:48,162 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=3&endTxId=10&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-01-19 14:30:48,228 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.06s at 16253.97 KB/s
2017-01-19 14:30:48,228 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000003-0000000000000000010_0000000000186023373 size 0 bytes.
2017-01-19 14:30:48,229 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=11&endTxId=12&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-01-19 14:30:48,262 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 0.00 KB/s
2017-01-19 14:30:48,262 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000011-0000000000000000012_0000000000186023440 size 0 bytes.
2017-01-19 14:30:48,298 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-01-19 14:30:48,332 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-01-19 14:30:48,332 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2 from /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000002
2017-01-19 14:30:48,332 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-01-19 14:30:48,337 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2017-01-19 14:30:48,340 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000010 expecting start txid #3
2017-01-19 14:30:48,341 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000010
2017-01-19 14:30:48,377 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000010 of size 1048576 edits # 8 loaded in 0 seconds
2017-01-19 14:30:48,377 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000011-0000000000000000012 expecting start txid #11
2017-01-19 14:30:48,378 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000011-0000000000000000012
2017-01-19 14:30:48,378 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000011-0000000000000000012 of size 42 edits # 2 loaded in 0 seconds
2017-01-19 14:30:48,382 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000012 using no compression
2017-01-19 14:30:48,416 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000012 of size 510 bytes saved in 0 seconds.
2017-01-19 14:30:48,465 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2017-01-19 14:30:48,465 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-01-19 14:30:48,603 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 12 to namenode at http://localhost:50070 in 0.095 seconds
2017-01-19 14:30:48,603 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 510
2017-01-19 16:06:02,296 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2017-01-19 16:06:02,308 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=13&endTxId=14&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-01-19 16:06:02,348 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2017-01-19 16:06:02,348 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000013-0000000000000000014_0000000000189624304 size 0 bytes.
2017-01-19 16:06:02,349 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-01-19 16:06:02,349 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000013-0000000000000000014 expecting start txid #13
2017-01-19 16:06:02,349 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000013-0000000000000000014
2017-01-19 16:06:02,349 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000013-0000000000000000014 of size 42 edits # 2 loaded in 0 seconds
2017-01-19 16:06:02,357 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000014 using no compression
2017-01-19 16:06:02,361 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000014 of size 510 bytes saved in 0 seconds.
2017-01-19 16:06:02,406 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 12
2017-01-19 16:06:02,406 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2017-01-19 16:06:02,523 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 14 to namenode at http://localhost:50070 in 0.086 seconds
2017-01-19 16:06:02,524 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 510
2017-01-19 20:16:51,284 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2017-01-19 20:16:51,321 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=15&endTxId=16&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-01-19 20:16:51,371 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2017-01-19 20:16:51,371 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000015-0000000000000000016_0000000000193225293 size 0 bytes.
2017-01-19 20:16:51,372 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-01-19 20:16:51,372 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000015-0000000000000000016 expecting start txid #15
2017-01-19 20:16:51,372 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000015-0000000000000000016
2017-01-19 20:16:51,373 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000015-0000000000000000016 of size 42 edits # 2 loaded in 0 seconds
2017-01-19 20:16:51,373 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000016 using no compression
2017-01-19 20:16:51,382 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000016 of size 510 bytes saved in 0 seconds.
2017-01-19 20:16:51,417 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 14
2017-01-19 20:16:51,417 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000012, cpktTxId=0000000000000000012)
2017-01-19 20:16:51,591 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 16 to namenode at http://localhost:50070 in 0.119 seconds
2017-01-19 20:16:51,591 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 510
2017-01-20 19:19:36,698 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2017-01-20 19:19:36,699 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=17&endTxId=18&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-01-20 19:19:36,740 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2017-01-20 19:19:36,740 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000017-0000000000000000018_0000000000196826090 size 0 bytes.
2017-01-20 19:19:36,741 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-01-20 19:19:36,741 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000017-0000000000000000018 expecting start txid #17
2017-01-20 19:19:36,741 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000017-0000000000000000018
2017-01-20 19:19:36,742 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000017-0000000000000000018 of size 42 edits # 2 loaded in 0 seconds
2017-01-20 19:19:36,744 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000018 using no compression
2017-01-20 19:19:36,753 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000018 of size 510 bytes saved in 0 seconds.
2017-01-20 19:19:36,787 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 16
2017-01-20 19:19:36,787 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000014, cpktTxId=0000000000000000014)
2017-01-20 19:19:36,942 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 18 to namenode at http://localhost:50070 in 0.112 seconds
2017-01-20 19:19:36,943 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 510
2017-01-20 20:19:37,832 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2017-01-20 20:19:37,868 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=19&endTxId=20&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-01-20 20:19:37,960 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2017-01-20 20:19:37,961 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000019-0000000000000000020_0000000000200427259 size 0 bytes.
2017-01-20 20:19:37,962 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-01-20 20:19:37,962 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000019-0000000000000000020 expecting start txid #19
2017-01-20 20:19:37,962 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000019-0000000000000000020
2017-01-20 20:19:37,963 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000019-0000000000000000020 of size 42 edits # 2 loaded in 0 seconds
2017-01-20 20:19:38,011 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000020 using no compression
2017-01-20 20:19:38,018 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000020 of size 510 bytes saved in 0 seconds.
2017-01-20 20:19:38,115 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 18
2017-01-20 20:19:38,115 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000016, cpktTxId=0000000000000000016)
2017-01-20 20:19:38,400 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 20 to namenode at http://localhost:50070 in 0.173 seconds
2017-01-20 20:19:38,400 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 510
2017-01-20 22:15:58,706 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-03 12:48:52,169 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-03 12:48:52,235 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-03 12:48:53,430 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-03 12:48:53,509 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-03 12:48:53,509 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-03-03 12:48:53,887 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/in_use.lock acquired by nodename 21783@gaurav-Inspiron-3542
2017-03-03 12:48:54,016 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-03 12:48:54,017 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-03 12:48:54,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-03 12:48:54,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-03 12:48:54,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-03 12:48:54,084 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 03 12:48:54
2017-03-03 12:48:54,087 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-03 12:48:54,087 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-03 12:48:54,090 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-03 12:48:54,090 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-03 12:48:54,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-03 12:48:54,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-03 12:48:54,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-03 12:48:54,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-03 12:48:54,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-03 12:48:54,122 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-03 12:48:54,122 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-03 12:48:54,122 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-03 12:48:54,124 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-03-03 12:48:54,124 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-03 12:48:54,124 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-03 12:48:54,124 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-03 12:48:54,127 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-03 12:48:54,203 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-03 12:48:54,204 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-03 12:48:54,204 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-03 12:48:54,204 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-03 12:48:54,205 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-03 12:48:54,205 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-03 12:48:54,205 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-03 12:48:54,205 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-03 12:48:54,224 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-03 12:48:54,224 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-03 12:48:54,225 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-03 12:48:54,225 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-03 12:48:54,227 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-03 12:48:54,227 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-03 12:48:54,227 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-03 12:48:54,232 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-03 12:48:54,232 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-03 12:48:54,232 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-03 12:48:54,248 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-03-03 12:48:54,367 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-03 12:48:54,384 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-03 12:48:54,393 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-03-03 12:48:54,403 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-03 12:48:54,406 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-03-03 12:48:54,406 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-03 12:48:54,406 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-03 12:48:54,434 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-03-03 12:48:54,435 INFO org.mortbay.log: jetty-6.1.26
2017-03-03 12:48:54,683 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-03-03 12:48:54,683 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-03-03 12:48:54,689 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-03-03 12:48:54,690 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-03-03 12:49:55,011 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2017-03-03 12:49:55,585 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=21&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-03-03 12:49:55,645 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2017-03-03 12:49:56,232 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2017-03-03 12:49:56,232 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000021 size 510 bytes.
2017-03-03 12:49:56,267 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=22&endTxId=23&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-03-03 12:49:56,302 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 0.00 KB/s
2017-03-03 12:49:56,302 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000022-0000000000000000023_0000000000058676859 size 0 bytes.
2017-03-03 12:49:56,362 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 3 INodes.
2017-03-03 12:49:56,407 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-03 12:49:56,407 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 21 from /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000021
2017-03-03 12:49:56,408 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-03 12:49:56,418 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-03-03 12:49:56,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000022-0000000000000000023 expecting start txid #22
2017-03-03 12:49:56,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000022-0000000000000000023
2017-03-03 12:49:56,435 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000022-0000000000000000023 of size 42 edits # 2 loaded in 0 seconds
2017-03-03 12:49:56,444 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000023 using no compression
2017-03-03 12:49:56,486 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000023 of size 510 bytes saved in 0 seconds.
2017-03-03 12:49:56,538 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 21
2017-03-03 12:49:56,538 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000020, cpktTxId=0000000000000000020)
2017-03-03 12:49:56,539 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000018, cpktTxId=0000000000000000018)
2017-03-03 12:49:56,685 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 23 to namenode at http://localhost:50070 in 0.105 seconds
2017-03-03 12:49:56,686 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 510
2017-03-03 14:33:53,235 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2017-03-03 14:33:53,272 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=24&endTxId=42&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-03-03 14:33:53,318 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 24.39 KB/s
2017-03-03 14:33:53,318 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000024-0000000000000000042_0000000000062277863 size 0 bytes.
2017-03-03 14:33:53,319 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-03-03 14:33:53,319 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000024-0000000000000000042 expecting start txid #24
2017-03-03 14:33:53,319 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000024-0000000000000000042
2017-03-03 14:33:53,402 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000024-0000000000000000042 of size 1676 edits # 19 loaded in 0 seconds
2017-03-03 14:33:53,403 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000042 using no compression
2017-03-03 14:33:53,406 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000042 of size 881 bytes saved in 0 seconds.
2017-03-03 14:33:53,501 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 23
2017-03-03 14:33:53,501 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000021, cpktTxId=0000000000000000021)
2017-03-03 14:33:53,654 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 42 to namenode at http://localhost:50070 in 0.091 seconds
2017-03-03 14:33:53,654 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 881
2017-03-03 18:42:51,880 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-03 18:42:51,968 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-04 22:09:21,082 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-04 22:09:21,113 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-04 22:09:21,781 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-04 22:09:21,858 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-04 22:09:21,858 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-03-04 22:09:22,150 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/in_use.lock acquired by nodename 6302@gaurav-Inspiron-3542
2017-03-04 22:09:22,246 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-04 22:09:22,247 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-04 22:09:22,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-04 22:09:22,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-04 22:09:22,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-04 22:09:22,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 04 22:09:22
2017-03-04 22:09:22,301 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-04 22:09:22,301 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-04 22:09:22,303 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-04 22:09:22,303 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-04 22:09:22,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-04 22:09:22,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-04 22:09:22,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-04 22:09:22,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-04 22:09:22,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-04 22:09:22,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-04 22:09:22,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-04 22:09:22,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-04 22:09:22,333 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-03-04 22:09:22,333 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-04 22:09:22,333 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-04 22:09:22,333 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-04 22:09:22,336 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-04 22:09:22,384 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-04 22:09:22,384 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-04 22:09:22,384 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-04 22:09:22,384 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-04 22:09:22,385 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-04 22:09:22,385 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-04 22:09:22,385 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-04 22:09:22,385 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-04 22:09:22,394 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-04 22:09:22,394 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-04 22:09:22,395 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-04 22:09:22,395 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-04 22:09:22,396 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-04 22:09:22,396 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-04 22:09:22,396 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-04 22:09:22,400 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-04 22:09:22,400 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-04 22:09:22,400 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-04 22:09:22,422 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-03-04 22:09:22,535 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-04 22:09:22,549 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-04 22:09:22,557 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-03-04 22:09:22,566 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-04 22:09:22,570 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-03-04 22:09:22,570 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-04 22:09:22,570 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-04 22:09:22,589 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-03-04 22:09:22,589 INFO org.mortbay.log: jetty-6.1.26
2017-03-04 22:09:22,797 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-03-04 22:09:22,797 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-03-04 22:09:22,800 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-03-04 22:09:22,800 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-03-04 22:47:26,744 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2017-03-04 22:47:29,850 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=43&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-03-04 22:47:29,955 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2017-03-04 22:47:31,662 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.08s at 0.00 KB/s
2017-03-04 22:47:31,663 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000043 size 881 bytes.
2017-03-04 22:47:31,786 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=44&endTxId=50&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-03-04 22:47:31,874 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.07s at 14628.57 KB/s
2017-03-04 22:47:31,874 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000044-0000000000000000050_0000000000003628383 size 0 bytes.
2017-03-04 22:47:31,875 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=51&endTxId=52&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-03-04 22:47:31,919 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2017-03-04 22:47:31,919 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000051-0000000000000000052_0000000000003628473 size 0 bytes.
2017-03-04 22:47:32,112 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 9 INodes.
2017-03-04 22:47:32,592 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-04 22:47:32,592 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 43 from /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000043
2017-03-04 22:47:32,593 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-04 22:47:32,642 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2017-03-04 22:47:32,647 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000044-0000000000000000050 expecting start txid #44
2017-03-04 22:47:32,647 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000044-0000000000000000050
2017-03-04 22:47:32,883 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000044-0000000000000000050 of size 1048576 edits # 7 loaded in 0 seconds
2017-03-04 22:47:32,883 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000051-0000000000000000052 expecting start txid #51
2017-03-04 22:47:32,883 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000051-0000000000000000052
2017-03-04 22:47:32,884 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000051-0000000000000000052 of size 42 edits # 2 loaded in 0 seconds
2017-03-04 22:47:32,915 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000052 using no compression
2017-03-04 22:47:33,017 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000052 of size 1208 bytes saved in 0 seconds.
2017-03-04 22:47:33,080 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 43
2017-03-04 22:47:33,080 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000042, cpktTxId=0000000000000000042)
2017-03-04 22:47:33,099 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000023, cpktTxId=0000000000000000023)
2017-03-04 22:47:33,388 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 52 to namenode at http://localhost:50070 in 0.212 seconds
2017-03-04 22:47:33,388 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1208
2017-03-04 22:49:34,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-04 22:49:35,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-04 22:49:36,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-04 22:49:37,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-04 22:49:38,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-04 22:49:39,299 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-04 22:49:39,301 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-06 12:45:50,885 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-06 12:45:50,927 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-06 12:45:51,613 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-06 12:45:51,692 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-06 12:45:51,692 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-03-06 12:45:52,015 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/in_use.lock acquired by nodename 30165@gaurav-Inspiron-3542
2017-03-06 12:45:52,111 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-06 12:45:52,111 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-06 12:45:52,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-06 12:45:52,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-06 12:45:52,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-06 12:45:52,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 06 12:45:52
2017-03-06 12:45:52,185 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-06 12:45:52,186 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-06 12:45:52,188 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-06 12:45:52,188 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-06 12:45:52,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-06 12:45:52,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-06 12:45:52,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-06 12:45:52,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-06 12:45:52,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-06 12:45:52,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-06 12:45:52,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-06 12:45:52,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-06 12:45:52,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-03-06 12:45:52,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-06 12:45:52,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-06 12:45:52,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-06 12:45:52,229 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-06 12:45:52,301 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-06 12:45:52,301 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-06 12:45:52,301 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-06 12:45:52,301 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-06 12:45:52,303 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-06 12:45:52,303 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-06 12:45:52,303 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-06 12:45:52,303 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-06 12:45:52,315 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-06 12:45:52,315 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-06 12:45:52,315 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-06 12:45:52,315 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-06 12:45:52,317 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-06 12:45:52,317 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-06 12:45:52,317 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-06 12:45:52,320 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-06 12:45:52,320 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-06 12:45:52,320 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-06 12:45:52,333 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-03-06 12:45:52,425 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-06 12:45:52,439 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-06 12:45:52,445 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-03-06 12:45:52,451 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-06 12:45:52,454 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-03-06 12:45:52,454 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-06 12:45:52,454 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-06 12:45:52,470 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-03-06 12:45:52,470 INFO org.mortbay.log: jetty-6.1.26
2017-03-06 12:45:52,682 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-03-06 12:45:52,683 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-03-06 12:45:52,693 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-03-06 12:45:52,693 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-03-06 12:46:42,567 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-06 12:46:42,569 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-06 12:48:01,716 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-06 12:48:01,728 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-06 12:48:02,679 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-06 12:48:02,831 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-06 12:48:02,831 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-03-06 12:48:03,167 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/in_use.lock acquired by nodename 32340@gaurav-Inspiron-3542
2017-03-06 12:48:03,277 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-06 12:48:03,277 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-06 12:48:03,370 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-06 12:48:03,370 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-06 12:48:03,372 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-06 12:48:03,375 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 06 12:48:03
2017-03-06 12:48:03,378 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-06 12:48:03,378 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-06 12:48:03,381 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-06 12:48:03,381 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-06 12:48:03,424 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-06 12:48:03,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-06 12:48:03,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-06 12:48:03,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-06 12:48:03,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-06 12:48:03,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-06 12:48:03,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-06 12:48:03,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-06 12:48:03,431 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-03-06 12:48:03,431 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-06 12:48:03,431 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-06 12:48:03,432 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-06 12:48:03,441 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-06 12:48:03,536 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-06 12:48:03,536 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-06 12:48:03,536 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-06 12:48:03,536 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-06 12:48:03,538 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-06 12:48:03,538 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-06 12:48:03,538 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-06 12:48:03,538 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-06 12:48:03,554 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-06 12:48:03,554 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-06 12:48:03,555 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-06 12:48:03,555 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-06 12:48:03,557 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-06 12:48:03,557 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-06 12:48:03,557 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-06 12:48:03,563 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-06 12:48:03,563 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-06 12:48:03,563 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-06 12:48:03,588 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-03-06 12:48:03,736 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-06 12:48:03,758 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-06 12:48:03,772 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-03-06 12:48:03,787 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-06 12:48:03,790 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-03-06 12:48:03,790 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-06 12:48:03,790 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-06 12:48:03,809 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-03-06 12:48:03,809 INFO org.mortbay.log: jetty-6.1.26
2017-03-06 12:48:04,026 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-03-06 12:48:04,026 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-03-06 12:48:04,028 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-03-06 12:48:04,028 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-03-06 12:49:04,405 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2017-03-06 12:49:05,825 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=53&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-03-06 12:49:05,891 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2017-03-06 12:49:06,501 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.09s at 10.75 KB/s
2017-03-06 12:49:06,501 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000053 size 1208 bytes.
2017-03-06 12:49:06,546 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=54&endTxId=54&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-03-06 12:49:06,613 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.06s at 16000.00 KB/s
2017-03-06 12:49:06,613 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000054-0000000000000000054_0000000000058883803 size 0 bytes.
2017-03-06 12:49:06,613 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=55&endTxId=56&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-03-06 12:49:06,646 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 0.00 KB/s
2017-03-06 12:49:06,646 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000055-0000000000000000056_0000000000058883871 size 0 bytes.
2017-03-06 12:49:06,728 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 14 INodes.
2017-03-06 12:49:06,793 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-03-06 12:49:06,793 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 53 from /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000053
2017-03-06 12:49:06,794 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-03-06 12:49:06,799 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2017-03-06 12:49:06,803 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000054-0000000000000000054 expecting start txid #54
2017-03-06 12:49:06,803 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000054-0000000000000000054
2017-03-06 12:49:06,842 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000054-0000000000000000054 of size 1048576 edits # 1 loaded in 0 seconds
2017-03-06 12:49:06,843 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000055-0000000000000000056 expecting start txid #55
2017-03-06 12:49:06,843 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000055-0000000000000000056
2017-03-06 12:49:06,844 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000055-0000000000000000056 of size 42 edits # 2 loaded in 0 seconds
2017-03-06 12:49:06,851 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000056 using no compression
2017-03-06 12:49:06,897 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000056 of size 1208 bytes saved in 0 seconds.
2017-03-06 12:49:06,940 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 53
2017-03-06 12:49:06,940 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000052, cpktTxId=0000000000000000052)
2017-03-06 12:49:06,952 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000043, cpktTxId=0000000000000000043)
2017-03-06 12:49:07,218 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 56 to namenode at http://localhost:50070 in 0.193 seconds
2017-03-06 12:49:07,218 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1208
2017-03-06 14:08:10,461 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2017-03-06 14:08:10,495 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=57&endTxId=58&storageInfo=-63:1805507603:0:CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b
2017-03-06 14:08:10,538 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2017-03-06 14:08:10,538 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000057-0000000000000000058_0000000000062485155 size 0 bytes.
2017-03-06 14:08:10,538 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2017-03-06 14:08:10,538 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000057-0000000000000000058 expecting start txid #57
2017-03-06 14:08:10,538 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000057-0000000000000000058
2017-03-06 14:08:10,539 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/edits_0000000000000000057-0000000000000000058 of size 42 edits # 2 loaded in 0 seconds
2017-03-06 14:08:10,539 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000058 using no compression
2017-03-06 14:08:10,544 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage.ckpt_0000000000000000058 of size 1208 bytes saved in 0 seconds.
2017-03-06 14:08:10,583 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 56
2017-03-06 14:08:10,583 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/current/fsimage_0000000000000000053, cpktTxId=0000000000000000053)
2017-03-06 14:08:10,742 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 58 to namenode at http://localhost:50070 in 0.103 seconds
2017-03-06 14:08:10,742 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1208
2017-03-06 14:40:51,893 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-06 14:40:51,896 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-06 14:51:45,337 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-06 14:51:45,349 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-06 14:51:46,350 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-06 14:51:46,436 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-06 14:51:46,436 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-03-06 14:51:46,748 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/in_use.lock acquired by nodename 7707@gaurav-Inspiron-3542
2017-03-06 14:51:46,854 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-06 14:51:46,854 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-06 14:51:46,917 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-06 14:51:46,917 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-06 14:51:46,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-06 14:51:46,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 06 14:51:46
2017-03-06 14:51:46,924 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-06 14:51:46,924 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-06 14:51:46,926 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-06 14:51:46,926 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-06 14:51:46,957 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-06 14:51:46,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-06 14:51:46,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-06 14:51:46,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-06 14:51:46,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-06 14:51:46,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-06 14:51:46,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-06 14:51:46,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-06 14:51:46,961 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-03-06 14:51:46,961 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-06 14:51:46,961 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-06 14:51:46,961 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-06 14:51:46,964 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-06 14:51:47,019 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-06 14:51:47,019 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-06 14:51:47,020 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-06 14:51:47,020 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-06 14:51:47,021 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-06 14:51:47,021 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-06 14:51:47,021 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-06 14:51:47,021 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-06 14:51:47,030 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-06 14:51:47,030 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-06 14:51:47,030 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-06 14:51:47,030 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-06 14:51:47,032 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-06 14:51:47,032 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-06 14:51:47,032 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-06 14:51:47,037 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-06 14:51:47,037 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-06 14:51:47,037 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-06 14:51:47,052 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-03-06 14:51:47,152 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-06 14:51:47,166 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-06 14:51:47,173 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-03-06 14:51:47,182 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-06 14:51:47,186 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-03-06 14:51:47,186 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-06 14:51:47,186 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-06 14:51:47,206 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-03-06 14:51:47,206 INFO org.mortbay.log: jetty-6.1.26
2017-03-06 14:51:47,438 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-03-06 14:51:47,438 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-03-06 14:51:47,678 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-03-06 14:51:47,679 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-03-06 14:52:48,175 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 14:53:48,315 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 14:54:48,465 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 14:55:48,644 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 14:56:48,792 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 14:57:49,043 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 14:58:49,194 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 14:59:49,345 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:00:49,506 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:01:49,647 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:02:49,788 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:03:49,958 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:04:50,086 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:05:50,202 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:06:50,315 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:07:50,515 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:08:50,664 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:09:50,783 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:10:50,905 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:11:51,072 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:12:51,203 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:13:51,331 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:14:51,462 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:15:51,613 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:16:51,729 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:17:51,845 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:18:52,006 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:19:52,132 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:20:52,281 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:21:52,478 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:22:52,615 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:23:52,732 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:24:52,870 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:25:52,952 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:26:53,059 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:27:53,153 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1478130905 cTime = 0 ; clusterId = CID-92f1b9f4-74b2-416f-8638-38a1bd55e982 ; blockpoolId = BP-2133005410-127.0.1.1-1488791586172.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-06 15:28:20,604 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-06 15:28:20,605 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-07 15:18:08,221 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-07 15:18:08,252 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-07 15:18:08,936 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-07 15:18:09,010 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-07 15:18:09,010 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-03-07 15:18:09,366 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/in_use.lock acquired by nodename 24918@gaurav-Inspiron-3542
2017-03-07 15:18:09,460 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-07 15:18:09,460 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-07 15:18:09,505 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-07 15:18:09,505 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-07 15:18:09,507 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-07 15:18:09,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 07 15:18:09
2017-03-07 15:18:09,511 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-07 15:18:09,511 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-07 15:18:09,512 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-07 15:18:09,513 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-07 15:18:09,540 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-07 15:18:09,541 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-07 15:18:09,541 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-07 15:18:09,541 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-07 15:18:09,541 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-07 15:18:09,541 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-07 15:18:09,541 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-07 15:18:09,541 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-07 15:18:09,544 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-03-07 15:18:09,544 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-07 15:18:09,544 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-07 15:18:09,544 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-07 15:18:09,547 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-07 15:18:09,593 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-07 15:18:09,593 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-07 15:18:09,593 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-07 15:18:09,593 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-07 15:18:09,594 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-07 15:18:09,594 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-07 15:18:09,595 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-07 15:18:09,595 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-07 15:18:09,602 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-07 15:18:09,602 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-07 15:18:09,603 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-07 15:18:09,603 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-07 15:18:09,604 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-07 15:18:09,604 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-07 15:18:09,604 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-07 15:18:09,608 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-07 15:18:09,608 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-07 15:18:09,608 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-07 15:18:09,625 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-03-07 15:18:09,696 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-07 15:18:09,707 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-07 15:18:09,714 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-03-07 15:18:09,720 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-07 15:18:09,724 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-03-07 15:18:09,724 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-07 15:18:09,724 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-07 15:18:09,743 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-03-07 15:18:09,743 INFO org.mortbay.log: jetty-6.1.26
2017-03-07 15:18:09,939 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-03-07 15:18:09,940 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-03-07 15:18:09,940 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-03-07 15:18:09,940 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-03-07 15:19:10,082 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 15:19:11,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:19:12,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:19:13,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:19:14,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:19:15,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:19:16,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:19:17,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:19:18,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:19:19,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:19:20,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:19:20,102 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 15:20:20,112 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 15:20:21,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:20:22,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:20:23,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:20:24,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:20:25,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:20:26,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:20:27,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:20:28,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:20:29,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:20:30,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:20:30,122 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 15:21:30,124 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 15:21:31,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:21:32,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:21:33,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:21:34,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:21:35,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:21:36,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:21:37,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:56:08,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:56:09,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:56:10,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:56:10,656 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 15:57:10,662 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 15:57:11,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:57:12,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:57:13,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:57:14,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:57:15,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:57:16,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:57:17,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:57:18,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:57:19,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:57:20,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:57:20,669 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 15:58:20,673 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 15:58:21,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:58:22,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:58:23,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:58:24,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:58:25,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:58:26,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:58:27,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:58:28,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:58:29,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:58:30,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:58:30,682 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 15:59:30,687 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 15:59:31,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:59:32,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:59:33,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:59:34,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:59:35,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:59:36,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:59:37,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:59:38,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:59:39,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:59:40,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 15:59:40,697 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 16:00:41,817 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:00:42,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:00:43,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:00:44,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:00:45,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:00:46,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:00:47,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:00:48,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:00:49,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:00:50,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:00:51,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:00:51,825 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 16:01:51,828 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:01:52,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:01:53,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:01:54,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:01:55,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:01:56,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:01:57,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:01:58,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:01:59,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:02:00,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:02:01,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:02:01,838 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 16:03:01,842 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:03:02,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:03:03,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:03:04,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:03:05,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:03:06,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:03:07,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:03:08,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:03:09,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:03:10,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:03:11,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:03:11,852 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 16:04:12,857 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:04:13,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:04:14,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:04:15,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:04:16,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:04:17,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:04:18,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:04:19,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:04:20,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:04:21,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:04:22,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:04:22,863 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 16:05:22,866 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:05:23,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:05:24,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:05:25,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:05:26,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:05:27,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:05:28,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:05:29,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:05:30,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:05:31,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:05:32,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:05:32,878 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 16:06:32,884 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:06:33,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:06:34,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:06:35,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:06:36,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:06:37,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:06:38,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:06:39,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:06:40,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:06:41,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:06:42,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:06:42,894 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 16:07:42,898 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:07:43,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:07:44,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:07:45,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:07:46,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:07:47,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:07:48,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:07:49,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:07:50,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:07:51,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:07:52,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:07:52,908 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 16:08:52,912 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:08:53,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:08:54,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:08:55,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:08:56,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:08:57,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:08:58,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:08:59,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:09:00,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:09:01,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:09:02,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:09:02,921 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 16:10:02,926 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:10:03,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:10:04,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:10:05,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:10:06,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:10:07,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:10:08,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:10:09,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:10:10,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:10:11,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:10:12,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:10:12,934 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 16:11:15,940 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:11:16,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:11:17,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:11:18,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:11:19,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:11:20,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:11:21,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:11:22,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:11:24,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:11:25,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:11:26,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:11:26,004 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-07 16:12:26,010 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:12:27,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:12:28,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:12:29,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:12:30,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:12:31,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:12:32,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:12:33,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:12:34,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:12:35,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:12:36,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:12:36,017 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:13:36,027 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:13:37,027 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:13:38,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:13:39,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:13:40,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:13:41,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:13:42,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:13:43,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:13:44,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:13:45,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:13:46,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:13:46,036 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:14:46,046 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:14:47,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:14:48,048 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:14:49,048 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:14:50,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:14:51,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:14:52,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:14:53,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:14:54,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:14:55,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:14:56,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:14:56,059 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:15:59,068 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:16:00,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:16:01,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:16:02,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:16:03,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:16:04,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:16:05,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:16:06,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:16:07,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:16:08,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:16:09,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:16:09,076 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:17:09,082 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:17:10,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:17:11,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:17:12,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:17:13,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:17:14,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:17:15,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:17:16,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:17:17,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:17:18,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:17:19,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:17:19,091 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:18:19,094 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:18:20,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:18:21,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:18:22,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:18:23,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:18:24,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:18:25,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:18:26,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:18:27,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:18:28,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:18:29,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:18:29,104 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:19:29,116 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:19:30,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:19:31,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:19:32,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:19:33,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:19:34,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:19:35,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:19:36,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:19:37,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:19:38,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:19:39,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:19:39,124 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:20:39,130 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:20:40,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:20:41,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:20:42,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:20:43,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:20:44,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:20:45,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:20:46,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:20:47,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:20:48,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:20:49,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:20:49,140 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:21:49,147 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:21:50,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:21:51,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:21:52,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:21:53,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:21:54,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:21:55,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:21:56,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:21:57,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:21:58,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:21:59,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:21:59,158 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:22:59,163 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:23:00,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:23:01,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:23:02,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:23:03,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:23:04,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:23:05,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:23:06,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:23:07,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:23:08,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:23:09,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:23:09,174 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:24:09,181 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:24:10,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:24:11,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:24:12,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:24:13,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:24:14,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:24:15,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:24:16,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:24:17,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:24:18,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:24:19,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:24:19,194 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:25:19,274 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:25:20,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:25:21,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:25:22,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:25:23,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:25:24,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:25:25,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:25:26,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:25:27,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:25:28,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:25:29,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:25:29,285 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:26:29,294 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:26:30,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:26:31,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:26:32,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:26:33,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:26:34,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:26:35,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:26:36,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:26:37,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:26:38,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:26:39,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:26:39,303 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:27:39,307 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:27:40,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:27:41,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:27:42,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:27:43,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:27:44,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:27:45,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:27:46,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:27:47,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:27:48,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:27:49,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:27:49,314 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:28:49,320 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:28:50,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:28:51,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:28:52,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:28:53,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:28:54,324 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:28:55,325 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:28:56,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:28:57,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:28:58,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:28:59,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:28:59,331 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:29:59,336 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:30:00,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:30:01,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:30:02,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:30:03,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:30:04,340 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:30:05,341 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:30:06,341 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:30:07,342 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:30:08,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:30:09,344 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:30:09,346 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:31:24,400 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:31:25,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:31:26,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:31:27,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:31:28,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:31:29,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:31:30,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:31:31,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:31:32,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:31:33,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:31:34,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:31:34,411 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:32:34,417 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:32:35,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:32:36,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:32:37,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:32:38,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:32:39,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:32:40,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:32:41,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:32:42,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:32:43,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:32:44,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:32:44,427 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:33:44,431 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:33:45,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:33:46,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:33:47,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:33:48,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:33:49,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:33:50,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:33:51,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:33:52,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:33:53,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:33:54,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:33:54,442 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:34:54,447 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:34:55,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:34:56,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:34:57,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:34:58,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:34:59,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:35:00,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:35:01,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:35:02,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:35:03,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:35:04,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:35:04,458 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:36:04,462 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:36:05,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:36:06,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:36:07,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:36:08,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:36:09,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:36:10,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:36:11,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:36:12,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:36:13,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:36:14,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:36:14,472 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:37:14,475 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:37:15,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:37:16,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:37:17,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:37:18,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:37:19,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:37:20,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:37:21,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:37:22,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:37:23,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:37:24,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:37:24,485 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:38:24,488 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:38:25,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:38:26,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:38:27,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:38:28,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:38:29,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:38:30,492 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:38:31,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:38:32,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:38:33,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:38:34,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:38:34,497 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:39:34,499 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:39:35,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:39:36,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:39:37,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:39:38,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:39:39,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:39:40,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:39:41,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:39:42,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:39:43,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:39:44,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:39:44,509 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:40:44,512 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:40:45,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:40:46,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:40:47,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:40:48,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:40:49,517 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:40:50,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:40:51,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:40:52,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:40:53,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:40:54,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:40:54,524 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:41:54,531 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:41:55,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:41:56,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:41:57,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:41:58,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:41:59,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:42:00,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:42:01,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:42:02,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:42:03,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:42:04,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:42:04,537 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:43:04,539 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:43:05,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:43:06,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:43:07,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:43:08,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:43:09,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:43:10,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:43:11,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:43:12,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:43:13,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:43:14,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:43:14,548 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:44:14,552 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:44:15,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:44:16,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:44:17,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:44:18,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:44:19,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:44:20,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:44:21,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:44:22,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:44:23,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:44:24,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:44:24,560 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:45:31,572 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:45:32,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:45:33,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:45:34,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:45:35,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:45:36,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:45:37,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:45:38,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:45:39,579 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:45:40,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:45:41,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:45:41,582 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:46:41,588 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:46:42,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:46:43,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:46:44,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:46:45,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:46:46,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:46:47,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:46:48,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:46:49,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:46:50,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:46:51,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:46:51,595 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:47:51,600 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:47:52,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:47:53,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:47:54,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:47:55,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:47:56,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:47:57,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:47:58,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:47:59,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:48:00,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:48:01,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:48:01,611 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:49:01,618 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:49:02,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:49:03,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:49:04,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:49:05,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:49:06,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:49:07,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:49:08,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:49:09,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:49:10,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:49:11,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:49:11,628 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:50:11,633 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:50:12,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:50:13,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:50:14,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:50:15,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:50:16,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:50:17,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:50:18,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:50:19,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:50:20,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:50:21,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:50:21,641 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:51:21,647 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:51:22,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:51:23,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:51:24,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:51:25,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:51:26,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:51:27,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:51:28,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:51:29,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:51:30,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:51:31,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:51:31,658 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:52:31,665 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:52:32,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:52:33,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:52:34,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:52:35,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:52:36,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:52:37,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:52:38,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:52:39,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:52:40,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:52:41,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:52:41,677 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:53:41,684 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:53:42,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:53:43,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:53:44,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:53:45,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:53:46,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:53:47,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:53:48,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:53:49,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:53:50,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:53:51,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:53:51,691 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:54:51,696 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:54:52,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:54:53,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:54:54,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:54:55,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:54:56,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:54:57,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:54:58,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:54:59,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:55:00,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:55:01,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:55:01,707 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:56:04,715 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:56:05,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:56:06,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:56:07,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:56:08,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:56:09,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:56:10,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:56:11,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:56:12,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:56:13,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:56:14,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:56:14,722 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:57:15,728 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:57:16,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:57:17,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:57:18,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:57:19,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:57:20,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:57:21,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:57:22,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:57:23,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:57:24,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:57:25,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:57:25,734 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:58:25,742 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:58:26,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:58:27,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:58:28,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:58:29,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:58:30,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:58:31,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:58:32,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:58:33,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:58:34,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:58:35,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:58:35,751 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 16:59:36,766 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 16:59:37,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:59:38,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:59:39,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:59:40,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:59:41,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:59:42,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:59:43,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:59:44,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:59:45,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:59:46,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 16:59:46,775 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:00:46,779 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:00:47,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:00:48,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:00:49,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:00:50,782 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:00:51,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:00:52,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:00:53,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:00:54,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:00:55,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:00:56,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:00:56,789 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:01:56,809 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:01:57,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:01:58,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:01:59,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:02:00,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:02:01,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:02:02,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:02:03,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:02:04,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:02:05,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:02:06,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:02:06,820 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:03:09,828 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:03:10,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:03:11,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:03:12,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:03:13,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:03:14,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:03:15,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:03:16,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:03:17,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:03:18,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:03:19,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:03:19,837 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:04:19,842 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:04:20,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:04:21,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:04:22,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:04:23,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:04:24,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:04:25,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:04:26,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:04:27,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:04:28,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:04:29,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:04:29,853 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:05:29,860 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:05:30,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:05:31,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:05:32,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:05:33,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:05:34,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:05:35,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:05:36,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:05:37,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:05:38,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:05:39,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:05:39,870 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:06:39,873 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:06:40,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:06:41,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:06:42,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:06:43,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:06:44,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:06:45,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:06:46,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:06:47,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:06:48,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:06:49,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:06:49,884 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:07:50,891 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:07:51,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:07:52,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:07:53,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:07:54,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:07:55,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:07:56,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:07:57,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:07:58,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:07:59,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:08:00,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:08:00,899 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:09:00,903 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:09:01,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:09:02,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:09:03,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:09:04,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:09:05,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:09:06,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:09:07,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:09:08,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:09:09,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:09:10,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:09:10,913 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:10:10,918 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:10:11,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:10:12,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:10:13,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:10:14,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:10:15,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:10:16,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:10:17,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:10:18,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:10:19,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:10:20,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:10:20,929 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:11:20,933 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:11:21,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:11:22,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:11:23,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:11:24,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:11:25,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:11:26,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:11:27,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:11:28,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:11:29,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:11:30,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:11:30,941 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:12:30,944 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:12:31,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:12:32,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:12:33,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:12:34,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:12:35,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:12:36,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:12:37,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:12:38,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:12:39,950 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:12:40,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:12:40,953 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:13:40,957 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:13:41,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:13:42,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:13:43,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:13:44,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:13:45,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:13:46,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:13:47,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:13:48,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:13:49,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:13:50,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:13:50,966 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:14:50,968 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:14:51,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:14:52,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:14:53,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:14:54,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:14:55,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:14:56,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:14:57,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:14:58,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:14:59,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:15:00,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:15:00,975 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:16:01,513 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:17:01,661 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:18:01,799 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:19:01,939 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:20:02,108 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:21:02,366 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:22:02,517 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:23:02,669 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:24:02,796 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:25:02,931 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:26:22,947 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:26:22,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); maxRetries=45
2017-03-07 17:26:23,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:26:24,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:26:25,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:26:26,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:26:27,950 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:26:28,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:26:29,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:26:30,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:26:31,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:26:32,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:26:32,957 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:27:33,098 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:28:33,243 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 911184934 cTime = 0 ; clusterId = CID-3857539d-d493-4531-8bcd-5cca92cf1df5 ; blockpoolId = BP-1243296677-127.0.1.1-1488886916340.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:29:33,347 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "gaurav-Inspiron-3542/127.0.1.1"; destination host is: "gaurav-Inspiron-3542":8020; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:773)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:520)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1084)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:979)
2017-03-07 17:30:33,374 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:30:34,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:30:35,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:30:36,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:30:37,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:30:38,379 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:30:39,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:30:40,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:30:41,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:30:42,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:30:43,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:30:43,385 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:31:43,388 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:31:44,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:31:45,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:31:46,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:31:47,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:31:48,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:31:49,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:31:50,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:31:51,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:31:52,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:31:53,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:31:53,398 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:32:53,732 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-07 17:33:53,734 WARN org.apache.hadoop.ipc.Client: Address change detected. Old: gaurav-Inspiron-3542/192.168.43.6:8020 New: gaurav-Inspiron-3542/127.0.1.1:8020
2017-03-07 17:33:54,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:33:55,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:33:56,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:33:57,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:33:58,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:33:59,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:34:00,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:34:01,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:34:02,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:34:03,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: gaurav-Inspiron-3542/127.0.1.1:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-07 17:34:03,745 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to gaurav-Inspiron-3542:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor5.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 17 more
2017-03-07 17:34:27,369 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-07 17:34:27,371 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-08 09:32:44,171 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-08 09:32:44,212 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-08 09:32:45,044 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-08 09:32:45,183 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-08 09:32:45,184 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-03-08 09:32:45,636 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/in_use.lock acquired by nodename 6180@gaurav-Inspiron-3542
2017-03-08 09:32:45,749 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-08 09:32:45,749 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-08 09:32:45,816 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-08 09:32:45,816 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-08 09:32:45,818 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-08 09:32:45,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 08 09:32:45
2017-03-08 09:32:45,822 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-08 09:32:45,822 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-08 09:32:45,824 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-08 09:32:45,824 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-08 09:32:45,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-08 09:32:45,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-08 09:32:45,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-08 09:32:45,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-08 09:32:45,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-08 09:32:45,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-08 09:32:45,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-08 09:32:45,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-08 09:32:45,877 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-03-08 09:32:45,877 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-08 09:32:45,877 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-08 09:32:45,877 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-08 09:32:45,881 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-08 09:32:45,981 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-08 09:32:45,981 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-08 09:32:45,981 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-08 09:32:45,982 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-08 09:32:45,982 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-08 09:32:45,983 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-08 09:32:45,983 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-08 09:32:45,983 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-08 09:32:45,990 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-08 09:32:45,990 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-08 09:32:45,990 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-08 09:32:45,990 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-08 09:32:45,992 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-08 09:32:45,992 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-08 09:32:45,992 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-08 09:32:45,996 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-08 09:32:45,997 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-08 09:32:45,997 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-08 09:32:46,037 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-03-08 09:32:46,132 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-08 09:32:46,148 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-08 09:32:46,153 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-03-08 09:32:46,158 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-08 09:32:46,161 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-03-08 09:32:46,161 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-08 09:32:46,161 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-08 09:32:46,211 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-03-08 09:32:46,211 INFO org.mortbay.log: jetty-6.1.26
2017-03-08 09:32:46,456 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-03-08 09:32:46,456 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-03-08 09:32:46,459 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-03-08 09:32:46,459 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-03-08 09:33:47,811 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:34:48,462 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:35:50,799 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:36:51,027 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:37:51,446 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:38:51,694 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:39:51,827 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:40:51,968 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:41:52,109 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:42:52,303 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:43:52,445 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:44:52,592 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:45:45,632 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-08 09:45:45,634 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-08 09:52:58,001 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-08 09:52:58,022 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-08 09:52:58,795 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-08 09:52:58,875 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-08 09:52:58,875 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-03-08 09:53:00,676 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/in_use.lock acquired by nodename 7968@gaurav-Inspiron-3542
2017-03-08 09:53:00,801 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-08 09:53:00,802 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-08 09:53:00,869 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-08 09:53:00,869 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-08 09:53:00,871 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-08 09:53:00,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 08 09:53:00
2017-03-08 09:53:00,881 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-08 09:53:00,881 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-08 09:53:00,884 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-08 09:53:00,884 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-08 09:53:00,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-08 09:53:00,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-03-08 09:53:00,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-08 09:53:00,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-08 09:53:00,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-08 09:53:00,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-08 09:53:00,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-08 09:53:00,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-08 09:53:00,927 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-03-08 09:53:00,927 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-08 09:53:00,927 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-08 09:53:00,927 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-08 09:53:00,931 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-08 09:53:00,984 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-08 09:53:00,984 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-08 09:53:00,985 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-08 09:53:00,985 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-08 09:53:00,986 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-08 09:53:00,986 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-08 09:53:00,986 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-08 09:53:00,986 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-08 09:53:00,996 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-08 09:53:00,996 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-08 09:53:00,996 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-08 09:53:00,996 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-08 09:53:00,998 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-08 09:53:00,998 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-08 09:53:00,998 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-08 09:53:01,003 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-08 09:53:01,003 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-08 09:53:01,003 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-08 09:53:01,017 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-03-08 09:53:01,109 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-08 09:53:01,125 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-08 09:53:01,134 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-03-08 09:53:01,146 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-08 09:53:01,152 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-03-08 09:53:01,152 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-08 09:53:01,153 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-08 09:53:01,180 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-03-08 09:53:01,180 INFO org.mortbay.log: jetty-6.1.26
2017-03-08 09:53:03,181 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-03-08 09:53:03,181 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-03-08 09:53:03,185 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-03-08 09:53:03,185 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-03-08 09:54:03,656 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:55:03,807 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:56:03,959 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 558480615 cTime = 0 ; clusterId = CID-ee392eba-af61-4ef3-a607-8188959a2933 ; blockpoolId = BP-7982064-127.0.1.1-1488888079552.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 09:57:04,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:57:05,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:57:06,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:57:07,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:57:08,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:57:09,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:57:10,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:57:11,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:57:12,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:57:13,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:57:13,974 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 09:58:14,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:58:15,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:58:16,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:58:17,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:58:18,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:58:19,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:58:20,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:58:21,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:58:22,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:58:23,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:58:23,984 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 09:59:24,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:59:25,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:59:26,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:59:27,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:59:28,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:59:29,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:59:30,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:59:31,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:59:32,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:59:33,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 09:59:33,993 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 10:00:34,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:00:35,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:00:36,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:00:37,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:00:38,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:00:39,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:00:41,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:00:42,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:00:43,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:00:44,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:00:44,004 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 10:01:45,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:01:46,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:01:47,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:01:48,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:01:49,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:01:50,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:01:51,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:01:52,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:01:53,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:01:54,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:01:54,015 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 10:02:55,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:02:56,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:02:57,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:02:58,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:02:59,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:03:00,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:03:01,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:03:02,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:03:03,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:03:04,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:03:04,024 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 10:04:05,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:04:06,027 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:04:07,027 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:04:08,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:04:09,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:04:10,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:04:11,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:04:12,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:04:13,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:04:14,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:04:14,037 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 10:05:15,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:05:16,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:05:17,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:05:18,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:05:19,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:05:20,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:05:21,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:05:22,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:05:23,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:05:24,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:05:24,048 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 10:06:25,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:06:26,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:06:27,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:06:28,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:06:29,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:06:30,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:06:31,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:06:32,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:06:33,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:06:34,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:06:34,061 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 10:07:35,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:07:36,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:07:37,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:07:38,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:07:39,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:07:40,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:07:41,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:07:42,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:07:43,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/192.168.43.6:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 10:07:43,416 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:08:43,549 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:09:43,692 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:10:43,824 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:11:43,951 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:12:44,128 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:13:44,260 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:14:44,418 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:15:44,551 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:16:44,710 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:17:44,856 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:18:45,003 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:19:45,178 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:20:45,325 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:21:45,460 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:22:45,597 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:23:45,765 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:24:45,920 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:25:46,052 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:26:46,200 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:27:46,345 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:28:46,488 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:29:46,624 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:30:46,780 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:31:46,947 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:32:47,082 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:33:47,246 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:34:47,401 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:35:47,544 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:36:47,800 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:37:47,957 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:38:48,103 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:39:48,235 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:40:48,378 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:41:48,538 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:42:48,668 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:43:48,802 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:44:48,922 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:45:49,058 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:46:49,225 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:47:49,548 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:48:49,685 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:49:49,820 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -63 namespaceID = 1197143511 cTime = 0 ; clusterId = CID-78bf6515-2a71-48e6-9fc0-fcbe64f3f8cc ; blockpoolId = BP-346729593-127.0.1.1-1488947804785.
Expecting respectively: -63; 1805507603; 0; CID-bdf8de9a-a250-4a10-9e64-973cee4a7d5b; BP-126370513-127.0.1.1-1484816050555.
	at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:134)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:531)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:50:49,835 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Log not rolled. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1327)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5832)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:512)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:51:49,849 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Log not rolled. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1327)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5832)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:512)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:52:49,864 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Log not rolled. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1327)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5832)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:512)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:53:49,877 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Log not rolled. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1327)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5832)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:512)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:54:49,942 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Log not rolled. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1327)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5832)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:512)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:55:50,018 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Log not rolled. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1327)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5832)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:512)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:56:50,060 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Log not rolled. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1327)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5832)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:512)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:57:50,097 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Log not rolled. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1327)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5832)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:512)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:58:50,110 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Log not rolled. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1327)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5832)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:512)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 10:59:50,117 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Log not rolled. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1327)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5832)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:512)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 11:00:50,129 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Log not rolled. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1327)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5832)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:512)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:395)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
2017-03-08 11:01:32,946 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-08 11:01:33,224 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
2017-03-08 11:52:37,283 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = gaurav-Inspiron-3542/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /home/hadoop1/Desktop/hadoop-2.7.3/etc/hadoop:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/home/hadoop1/Desktop/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar:/home/hadoop1/Desktop/hadoop-2.7.3/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.7.0_111
************************************************************/
2017-03-08 11:52:37,319 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-03-08 11:52:38,211 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-03-08 11:52:38,284 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-03-08 11:52:38,284 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2017-03-08 11:52:38,601 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/hadoop1/Desktop/hadoop-2.7.3/hadoop2_data/hdfs/temp/dfs/namesecondary/in_use.lock acquired by nodename 6196@gaurav-Inspiron-3542
2017-03-08 11:52:38,707 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-03-08 11:52:38,708 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-03-08 11:52:38,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-03-08 11:52:38,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-03-08 11:52:38,783 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-03-08 11:52:38,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Mar 08 11:52:38
2017-03-08 11:52:38,786 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-03-08 11:52:38,787 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-08 11:52:38,801 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-03-08 11:52:38,801 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-03-08 11:52:38,840 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-03-08 11:52:38,840 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 2
2017-03-08 11:52:38,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-03-08 11:52:38,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-03-08 11:52:38,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-03-08 11:52:38,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-03-08 11:52:38,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-03-08 11:52:38,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-03-08 11:52:38,844 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hadoop1 (auth:SIMPLE)
2017-03-08 11:52:38,844 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-03-08 11:52:38,844 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-03-08 11:52:38,844 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-03-08 11:52:38,848 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-03-08 11:52:38,908 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-03-08 11:52:38,909 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-08 11:52:38,909 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-03-08 11:52:38,909 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-03-08 11:52:38,910 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-03-08 11:52:38,910 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-03-08 11:52:38,910 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-03-08 11:52:38,910 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-03-08 11:52:38,919 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-03-08 11:52:38,919 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-03-08 11:52:38,919 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-03-08 11:52:38,919 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-03-08 11:52:38,921 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-03-08 11:52:38,921 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-03-08 11:52:38,921 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-03-08 11:52:38,924 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-03-08 11:52:38,924 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-03-08 11:52:38,925 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-03-08 11:52:38,953 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2017-03-08 11:52:39,039 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-03-08 11:52:39,053 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-03-08 11:52:39,059 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2017-03-08 11:52:39,065 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-03-08 11:52:39,068 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2017-03-08 11:52:39,068 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-03-08 11:52:39,068 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-03-08 11:52:39,083 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2017-03-08 11:52:39,083 INFO org.mortbay.log: jetty-6.1.26
2017-03-08 11:52:39,250 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2017-03-08 11:52:39,250 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2017-03-08 11:52:39,254 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2017-03-08 11:52:39,254 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2017-03-08 11:53:40,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:53:41,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:53:42,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:53:43,416 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:53:44,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:53:45,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:53:46,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:53:47,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:53:48,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:53:49,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:53:49,573 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 11:54:50,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:54:51,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:54:52,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:54:53,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:54:54,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:54:55,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:54:56,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:54:57,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:54:58,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:54:59,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:54:59,678 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 11:56:00,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:56:01,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:56:02,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:56:03,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:56:04,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:56:05,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:56:06,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:56:07,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:56:08,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:56:09,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:56:09,776 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 11:57:10,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:57:11,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:57:12,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:57:13,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:57:14,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:57:15,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:57:16,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:57:17,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:57:18,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:57:19,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:57:19,901 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 11:58:20,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:58:21,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:58:22,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:58:23,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:58:24,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:58:25,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:58:26,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:58:27,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:58:29,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:58:30,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:58:30,026 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 11:59:31,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:59:32,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:59:33,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:59:35,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:59:36,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:59:37,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:59:38,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:59:39,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:59:40,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:59:41,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 11:59:41,208 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 12:00:42,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:00:43,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:00:44,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:00:45,247 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:00:46,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:00:47,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:00:48,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:00:49,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:00:50,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:00:51,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:00:51,307 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 12:01:52,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:01:53,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:01:54,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:01:55,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:01:56,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:01:57,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:01:58,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:01:59,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:02:00,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:02:01,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop/192.168.43.107:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-03-08 12:02:01,519 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From gaurav-Inspiron-3542/127.0.1.1 to hadoop:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
	at org.apache.hadoop.ipc.Client.call(Client.java:1451)
	... 18 more
2017-03-08 12:02:45,765 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2017-03-08 12:02:45,766 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at gaurav-Inspiron-3542/127.0.1.1
************************************************************/
